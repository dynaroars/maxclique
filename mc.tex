\documentclass[11pt]{article} 
\usepackage[dvips]{graphicx} 
%\usepackage[bf]{caption2} 
\usepackage{amsmath}
\usepackage[figure,ruled,vlined]{algorithm2e}
\setlength{\textwidth}{6in} 
\setlength{\textheight}{8.5in} 
\setlength{\oddsidemargin}{0.2in}
\setlength{\evensidemargin}{0.5in}
\setlength{\topskip}{0in} 
\setlength{\topmargin}{0in} 
\setlength{\headsep}{0in}

\newcommand{\myfont}[1]{{\usefont{OT1}{cmss}{m}{it}#1}}
  
\begin{document}  

\title{Implementing the Max-Clique Ant System Algorithm in Parallel Environments} 
\author{
Authors\\
}  
\maketitle  
% Computer Science Program\\ 
% Penn State Harrisburg\\ 
% Middletown, PA 17057\\
 

\begin{abstract}
In this paper we re-implement the Ant System algorithm for the Max Clique problem~\cite{BR} in the shared and distributed parallel environments. Our parallel versions provide noticeable improvements to the algorithm running time while maintain comparable solution quality to that of the sequential version.  This shows Ant System based algorithms are promising candidates to parallel computing.
\end{abstract}


\section{Introduction}\label{intro}


Many real world applications bear resemblances to  $\cal NP$-hard combinatorial optimization problems.  Hence, unless $\cal P = NP$, they most likely have no algorithms that give exact or even good approximate solutions in acceptable time.  Alternative strategies such as ant based heuristics  have been shown to be very successful in these situations, even though they give no guarantees about solution quality.  However, in many cases ant based approaches still require large computing resources and time, especially when the problem size increases.  Furthermore, algorithms inspired by the social work of ants should  include their inherent synchronous operations.  These reasons provide incentives for studies about ant based algorithms in parallel environments.

In~\cite{BR}, Bui et al. used a variant of ant based algorithms called \textit{Ant System} to solve the \textit{Max-Clique} problem. Their conclusion offered brief intuitions on an implementation of the algorithm in distributed computing.  In this paper, we describe the process of transforming their sequential algorithm into two popular parallel architectures, namely the shared and distributed memory models. We hope these ideas serve as proofs of concept that parallel programming is ideal for algorithms inspired by social insects. %todo check et al


The rest of the paper is organized as follows. In Section~\ref{background} we give preliminaries on ant based algorithms, the clique problem, and an overview of parallel computing.  We present the sequential and parallel Ant System algorithms for Max-Clique in Section~\ref{algorithm} and show our experimental results in Section~\ref{results}.  Finally, our conclusion and suggestions for future work are given in Section~\ref{conclusion}.

%, and the two parallel environments considered in this work

\section{Background}\label{background} 
\subsection{The \textit{Max-Clique} Problem}

A \textit{clique} in an undirected graph $G$ is a complete subgraph of $G$ in which there's an edge between any two vertices in the subgraph.  The size of the clique is the number of vertices it owns. The  Max-Clique optimization problem asks for the \textit{largest} clique in a graph.  Max-Clique is very useful in solving real-life situations such as detecting good codes, identifying faulty processors in multiprocessor systems, and finding counterexamples to Keller's conjecture in geometry~\cite{BP2}\cite{Sloane}\cite{SMW}\cite{Keller}\cite{LS}.

It has been shown that even approximation algorithms for Max-Clique in polynomial time are impractical as the margin of error can grow quite high~\cite{Hastad}. Therefore, heuristics such as ant based algorithms are often used to find good solutions in a reasonable amount of time.

\subsection{Ant based Algorithms}\label{max_clique} 


\textit{Ant Colony Optimization} (ACO) is an eminent ant inspired approach that imitates the collective behavior of ants to solve problems~\cite{DD}. Ants in a colony communicate among themselves through pheromone and they are able to find the shortest path to a food source by marking their trails through this chemical substance~\cite{BDT}. Ants lay more or less pheromone according to how they perceive their conditions at a given time. Other ants arriving at a similar situation consider the amount of pheromone that has been laid, and use this to make informed decisions. ACO has been used to solve problems like traveling salesman, quadratic assignment, routing, knapsack, among others~\cite{DG, TAIL, MC}.

\textit{Ant System} (AS) is another ant based algorithm that has been effectively applied to graph like problems such as the bisection, $k$-cardinality tree, and degree-constrained spanning tree problems~\cite{BS2, BS1, BZ}.  AS differs from ACO mainly in how the ants are used. Each ant produces a solution to the problem in ACO. The initial ant solves the problem almost purely stochastically. The subsequent ants use the information provided by the previous ants to produce better solutions. In AS, ants make heuristic decisions based only on local neighborhood information and do not attempt to solve the problem individually. The solution to the problem is represented by the state of all ants in the colony captured in a snapshot of time. Because the ants are concerned with only a local portion of the problem, they need to know less global information than ants in an ACO algorithm. Moreover, since the solutions are obtained by groups of ants, AS employs lots more ants than ACO. These reasons make AS algorithms very amenable to parallel processing. 


\subsection{The Shared and Distributed Memory Models}


Shared and distributed memory are two prevalent inter-processor communication systems in parallel computing~\cite{PH}. Systems using shared memory (SM) such as symmetric multiprocessor (SMP) allow a collection of identical processors to share access to common memory, typically via a bus.  Distributed memory (DM) systems such as Beowulf clusters compose of multiple standalone machines each with its own processor and memory set. These machines are usually asymmetric and communicate with one another by means of passing messages though a high-speed network~\cite{cluster}.  POSIX Threads and OpenMP (MultiProcessing) are two standard shared memory API's and Message Passing Interface (MPI) is the traditional API in message passing system~\cite{MPI,OpenMP}.


Both shared-memory multiprocessors and distributed-memory processors have advantages and disadvantages. Porting a serial program to SM using API's such as OpenMP often just requires adding parallelism to appropriate sections of the sequential code.  However, race conditions, deadlocks, and other problems associated with shared access might occur.  Writing message passing programs in DM is more complicated; typically involves designing efficient algorithms to divide tasks among processors with separate memory spaces.  Moreover, processes coordinations, data synchronizations, and especially the latency of the network connecting the DM processors are common challenges.  The major advantage of DM over SM is scalability.  Adding more processors in a distributed environment is as simple as attaching new computers to the current environment, whereas doing so in shared memory setting increases the bus traffic on the system, slowing down memory access time, and delaying program execution~\cite{}.

\subsubsection{Previous Work}

Existing parallel ant algorithms often fall under the classes of parallel ants, parallel ant colonies, or hybridization of various parallel techniques~\cite{TRFR,RL,CR,B1,MRS,DHKLR,Stutzle,MBSD}.  In parallel ant, each ant occupies a separate processor and sends pheromone update to others at each step of the algorithm. A colony of ants is housed by a processor in the parallel ant colonies model and the program periodically broadcasts the pheromone structure from the best performing colony to others.  The hybrid model suggests combining parallel ant or parallel ant colonies with other parallel techniques. For instance, at the end of each algorithm the ants examine the available solutions elements; these expensive and independent evaluations can be divided equally to the available processors for parallel processing.

An OpenMP implemented model proposed creating multiple data structures (e.g., solution trees,  pheromone matrix) and assigning each to a thread (processor)~\cite{PKGG}.  These threads work independently on their assigned data structures and merge them together when needed.  However there are potential issues with managing data structures with dynamically allocated values (e.g., insert and delete nodes from trees).

With the exception of~\cite{PKGG}, the above approaches assume the more popular distributed memory model rather than the shared memory one. In addition, mainly Ant Colony Optimization based algorithms are used in these studies.



\section{Ant System Algorithm for Max-Clique (ASMC)}\label{algorithm}
\subsection{The Sequential Implementation (ASMC$_{SEQ}$)}\label{ASMC_seq} 
\begin{center} 
\begin{algorithm}
\SetKwFor{For}{for}{do}{endfor}
\SetKwComment{Comment}{//}{}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\dontprintsemicolon

\SetLine
% {\bf{Algorithm ABAC}}$(G=(V,E))$  
\Input{Graph $G=(V,E)$}
\Output{A \myfont{clique} of $G$}
\medskip
\Begin{
  \medskip
  use a \myfont{greedy} method to get a clique $q$ in $G$\;
  distribute ants on the vertices of $G$ with heavier concentration in $q$\; 
  \medskip
  \For{$s = 1$ {\bf{to}} \myfont{nStages}}{
    \For{$c = 1$ {\bf{to}} \myfont{nCycles}}{
      \For{$a = 1$ {\bf{to}} \myfont{nAnts}}{
        ant $a$ decides where to move based on surrounding information\;
        ant $a$ moves to its determined destination and deposits pheromone on the way\;
        small amount of pheromone \myfont{evaporates} from areas close to ant $a$'s previous location\;
      }
    }

    \medskip
    find cliques through \myfont{local optimization}\;
    \myfont{shuffle} ants\;
  }

  \medskip
  \Return{best} clique found
  \medskip
}
\caption{The sequential Ant System algorithm for Max-Clique (ASMC$_{SEQ}$)\label{fig:asmc_seq}} 
\end{algorithm}

\linespread{1.3}  
\end{center} 

The main idea of the algorithm given in Figure~\ref{fig:asmc_seq} is as follows.  Ants are distributed on the graph vertices.  Each ant follows the same set of rules to move from vertex to vertex along the edges of the graph.  The rules are designed to encourage ants to aggregate on sets of vertices that are highly connected.  These highly connected regions of the graph then serve as candidate sets of vertices from which we can construct cliques.

The algorithm starts with a \textit{greedy-based} algorithm to quickly find a clique for the ants to start with.  A large fraction of the ants are then distributed at random on vertices of the clique.  The algorithm then goes through a number of states, each state in turn consists a number of cycles and at the end of which a clique is constructed. In each cycle a fraction of the ants are selected to move and they do so with certain probability.  Their destinations are determined by various factors such as pheromone concentration and structure of the neighborhood around the ants.  As an ant traverses an edge of the graph it also puts down a certain amount of pheromone on that edge.  To allow for more exploration and possible escape from local optima, pheromone evaporates over time.  In addition to using pheromone as a means of communication, ants in our system also use their positions to communicate.  For example, a vertex currently occupied by a large number of ants is more attractive to an ant to go to. At the end of each stage, the algorithm performs a \textit{local optimization} technique which constructs a clique based on the current configuration of the ants. A small portion of ants is then shuffled around the graph before moving on to the next stage. After finishing all stages, the algorithm returns the largest clique found in all stages. %todo: shorten the part about ant movements


Techniques such as pheromone evaporation and ants shuffle are used mitigate problems caused by premature convergences and local optima traps. Moreover, adaptive behaviors are given to ants to encourage a gradual transformation from exploration in the beginning to exploitation near the end. In early stages, young ants are more active and rely less on pheromone and more on the structure of the graph.  As the algorithm progresses, ants get older and are less likely to move and they make more use of pheromone in determining their movement.
%todo: shorten

More detailed descriptions of each component in ASMC can be found in~\cite{BR}. We note even though the steps given here are for the \textit{clique} problem, most ideas are general and applicable to other Ant System based algorithms. 

\subsection{The Shared Memory Implementation (ASMC$_{SM}$)}\label{ASMC_sm}



\begin{center} 
  \begin{algorithm}
    \SetKwFor{For}{for}{do}{endfor}
    \SetKwComment{Comment}{//}{}
    \dontprintsemicolon

    \SetLine
    \Begin{
      \For{$c = 1$ {\bf{to}} \myfont{nCycles}}{
        \medskip
        \Comment{\myfont{Parallel region}: ants make decisions}

        \For{$a = 1$ {\bf{to}} \myfont{nAnts}}{
          ant $a$ decides where to move based on surrounding information\;

        }

        \medskip
        \medskip

        \Comment{\myfont{Parallel region}: ants move}
        \For{$a = 1$ {\bf{to}} \myfont{nAnts}}{
          ant $a$ moves to its determined destination and deposits pheromone on the way\;
          areas closed to ant $a$'s previous location are marked for pheromone evaporation\;
        }
        \medskip
        
        \Comment{\myfont{Sequential region}}
        evaporates small amount of pheromone from the marked areas\;
        \medskip
      }

    }
    \caption{Parallel Ant Operations in ASMC$_{SM}$\label{fig:par_ops}} 
  \end{algorithm}

  \linespread{1.3}  
\end{center}

The common programming paradigm for shared memory architecture is the multithreaded \textit{fork-join} model. A single \textit{Master} thread is created when the program starts, executes sequentially, and expands into multiple \textit{Slave} threads when parallelism is requested. These threads work concurrently through the parallel region then merge back to the single master thread when done with the parallel region.

To use the \textit{fork-join} model, parallel regions from the sequential code need to be specified. Sections in ASMC that are efficient and safe for parallelism, i.e., reducing the execution time while not degrading the solution quality, make good candidates. The \textit{stage} and \textit{cycle} loops do not fit these criteria since each stage requires information from previous stage and an ant can only perform an action (movement) per time cycle.  We then designate the \textit{ant} section as the main parallel regions since parallelizing this part does not affect the quality of the results.  Moreover, the size of the ant colony is approximately six times the number of vertices of the input graph (AS algorithms often use a much larger ant population than traditional ACO).  Consequently, the work of the ants consumes more than half of the total time execution in ASMC~\footnote{The GNU profile utility \textit{grof} was used to evaluate the complexity of our program.}.

In each cycle, an ant determines where to move and moves there.  These activities are expensive since the ant examines the surrounding neighborhood to decide its next destination, deposits pheromone on the way, and updates its new position.  In the sequential version, the actions of an ant have influences on the decisions of the other ants, even they occur in same time cycle. We redesign this part by separating it into two operations and parallelize them both as illustrated in Figure~\ref{fig:par_ops}.  This refinement allows the ants make decisions based on the current configuration and independent from other ants in the same time cycle.  Hence, in addition to speed gain, the revision provides better imitation of the inherent parallelism from natural ants activities.

Upon the departure of an ant, a small amount of pheromone is vaporized from its surrounding regions. This evaporation only happens on an edge once per cycle. This procedure is time costly since the algorithm checks if the edges adjacent to the location of the ant need updates whenever it moves. Moreover, this part cannot be concurrently processed as simultaneous updates to the same area can occur due to movements from ants in nearby regions.

As shown in Figure~\ref{fig:par_ops}, we resolve this situation by having the selection of which regions to be updated done in parallel, but the actual updating process is done in sequence.  A shared Boolean array of size equals to the number of edges is created with everything initialized to \textit{false}.  When the pheromone on an edge is determined to be updated, the corresponding value of that edge in the array is marked as \textit{true}.  Hence, even if simultaneous updates occur on the edge, its value still remains the same\footnote{Alternatively we can create multiple copies of these arrays, assign each to a parallel thread, and merge the results for the sequential update.}. After all ants make their moves, the pheromone information on the marked edges in the array are updated sequentially.  Note we also could parallel this step, however experiments show this simple operation gain no significant improvement from parallel processing.

Almost any loop section in the sequential code can be executed in parallel with the \textit{fork-join} model; however not all make good choices for optimizations.  Low complexity tasks, such as shuffling ants, may worsen the overall running time as the overheads caused by forking and joining threads surpass the speed gained from parallel processing.  Nonetheless, even without speed improvement, in some cases parallelism helps enhance the solution quality, thus indirectly reduces the running time by making the algorithm converges faster. Instead of parallelizing the already efficient greedy and local optimization algorithms, we independently process them in parallel and choose the best answers.  For example, multiple instances of greedy algorithms parallel processes with different selection criteria and local optimization simultaneously examines multiple candidate regions on the graph. Our experiments show on average the algorithm achieves good solutions in less iterations when combined with these techniques.

\subsection{The Distributed Memory Implementation (ASMC$_{DM}$)}\label{ASMC_dist} 

Unlike in ASMC$_{SM}$ where straightforward modifications can be applied directly to the sequential algorithm to achieve parallelism, we often need to redesign the sequential algorithm to get the full benefits of distributed memory architecture. 

The distributed implementation built here is a simple proof of concept based on the \textit{server/client} model.  Ants are distributed across a number of machines, each of which has a complete copy of the graph.  The idea is for ants to move from processor to processor.  One machine is designated as server and the others as clients.  The server coordinates four transactions: starting, ant transfer, local optimization, and ending.


\begin{itemize}

\item To {\bf start}, the server first waits for each client to connect.  It then partitions the vertices into four roughly equal sets and assigns each set to a processor, including itself.  It sends each client a list of which vertices are ``owned'' by which processors.

\item {\bf Ant transfer} occurs at the end of each stage.  The server asks each client for a list of ants that are moving to other processors.  It sorts these ants according to their destinations and sends each client a list of incoming ants.  Each processor instantiates the ants on its copy of the graph. It also updates cached data regarding vertices and edges connected to its ``owned'' vertices.  The cached data makes it possible for ants to make informed decisions about whether to move to another processor in the future.

\item {\bf Local optimization} occurs after each ant transfer.  Each client sends the server vertex and edge data, and the server performs the same operations as in the sequential implementation of ASMC.  In principle, this step could be redesigned to be less centralized.

\item The server {\bf ends} by letting each client know that the run has ended.  %Synchronized starting and ending makes it simpler to invoke the program from a script.

\end{itemize}



\section{Experimental Results}\label{results}

\linespread{1}
\begin{table*}[ht!]
\caption{ASMC$_{SM}$ results}\label{tab:spar_table}
\begin{footnotesize}
\begin{center}
\begin{tabular}{|l||c|c||c|c|c|c|c|}
\hline%todo have to explain P Best -- use a dag or something
&&ASMC& \multicolumn{5}{|c|}{Solution Avg/StdDev when run on $P$ processors}\\
\cline{4-8}
Graph &Opt & Best      &$P_1$(Seq)  &$P_2$ &$P_4$   &$P_8$    &$P_{16}$\\
\hline											            				       
c-fat200-1 	&12 &12	   &12.00/0.00 &12.00/0.00 &12.00/0.00 &12.00/0.00 &12.00/0.00\\
c-fat500-1 	&14 &14	   &14.00/0.00 &14.00/0.00 &14.00/0.00 &14.00/0.00 &12.00/0.00\\
%% \hline
johnson16-2-4 	&8 	&8 	   &8.00/0.00 &8.00/0.00 &8.00/0.00 &8.00/0.00 &8.00/0.00\\
johnson32-2-4 	&16 &16    &16.00/0.00 &16.00/0.00 &16.00/0.00 &16.00/0.00 &16.00/0.00\\
\hline 
keller4 &11 &11 	&10.22/0.83 &10.20/0.87 &10.35/0.84 &10.24/0.86 &10.44/0.82\\
keller5 &27 &24     &21.28/0.87 &21.45/0.95 &21.37/0.92 &21.4/0.82 &21.46/8.89\\
%% \hline
hamming10-2 &512 &512 &512.00/0.00 &512.00/0.00 &512.00/0.00 &512.00/0.00 &512.00/0.00\\
hamming8-2 	&128 &128 &128.00/0.00 &128.00/0.00 &128.00/0.00 &128.00/0.00 &512.00/0.00\\
\hline
san200\_0.7\_1 	&30 &30 &20.98/5.95 &21.4/6.09 &21.4/5.99 &19.24/4.62 &20.43/5.53\\
san200\_0.9\_1 	&70 &70 &45.52/0.67 &45.58/0.62 &45.64/0.76 &45.51/0.66 &45.64/0.70\\
san200\_0.9\_2 	&60 &60 &39.59/2.31 &39.58/2.75 &39.56/2.62 &39.84/3.07 &39.99/4.21\\
san200\_0.9\_3 	&44 &37 &34.61/0.90 &34.83/0.98 &34.82/0.92 &34.88/1.14 &34.91/1.08\\
san400\_0.5\_1 	&13 &13 &8.67/0.79 &8.72/0.90 &8.69/0.78 &8.73/0.79 &8.63/0.66\\
san400\_0.9\_1 	&100 &100 &68.25/20.35 &75.66/22.44 &74.19/21.77 &73.01/21.79 &71.44/21.69\\
\hline
sanr200\_0.7 	&18 &18  &17.27/0.55 &17.22/0.50 &17.29/0.62 &17.34/0.62 &17.34/0.51\\
sanr400\_0.5 	&13 &13  &12.06/0.40 &12.06/0.37 &12.01/0.41 &12.01/0.48 &11.98/0.47\\
san1000 	    &15 &10  &9.94/0.24 &9.96/0.20 &9.94/0.24 &9.93/0.26 &9.97/0.17\\
\hline
brock200\_1 	&21 &21  &19.48/0.52 &19.5/0.50 &19.55/0.54 &19.57/0.53 &19.64/0.52\\
brock400\_1 	&27 &25  &22.71/0.73 &22.75/0.67 &22.82/0.75 &22.82/0.74 &22.75/0.82\\
brock800\_1 	&23 &21  & 18.49/0.62 &18.61/0.66 &18.59/0.60 &18.7/0.66 &18.67/0.65\\
\hline
p\_hat300\_1 	&8 	&8   &8.00/0.00 &7.99/0.10 &7.99/0.10 &7.99/0.1 &8.00/0.00\\
p\_hat300\_2 	&25	&25  &25.00/0.00 &25.00/0.00 &25.00/0.00 &25.00/0.00 &25.00/0.00\\
p\_hat300\_3 	&36 &36  &34.89/0.87 &35.1/0.89 &35.01/0.89 &35.12/0.86 &34.97/0.98\\
p\_hat500\_1 	&9 	&9   &8.94/0.24 &8.98/0.14 &9.00/0.00 &8.98/0.14 &9.00/0.00 \\
p\_hat500\_2 	&36 &36  &35.96/0.20 &35.98/0.14 &35.92/0.27 &35.93/0.26 &35.93/0.26\\
p\_hat700\_1 	&11 &11  &9.31/0.61 &9.31/0.52 &9.27/0.55 &9.37/0.67 &9.24/0.51\\
p\_hat1000\_1 	&10 &10  &9.95/0.22 &9.99/0.10 &9.98/0.14 &9.98/0.14 &9.94/0.24\\
p\_hat1500\_1 	&12 &11	 &10.67/0.47 &10.72/0.45 &10.66/0.47 &10.72/0.45 &10.73/0.44\\
\hline
MANN\_a27 	&126 &126 &125.00/0.00 &125.01/0.10 &125.00/0.00 &125.01/0.10 &125.00/0.00\\
MANN\_a45 	&345 &342 &342.00/0.00 &342.00/0.00 &342.00/0.00 &342.00/0.00 &342.00/0.00 \\
\hline
%\multicolumn{8}{@{}l}{\tiny $^*$Results for 100 runs per graph.}
%\qquad\qquad $^\dag$All times are in seconds.}\\
%\multicolumn{9}{@{}l}{\tiny $^\dag$All times are in seconds.}\\
\end{tabular}
\end{center}
\end{footnotesize}
\end{table*}
\linespread{1.3}

\linespread{1}
\begin{table*}[ht!]
%\caption{Time and Speedup$^{*\dag}$\label{tab:spar_su_par_table}}
\caption{ASMC$_{SM}$ Time and Speed-up results}\label{tab:spar_su_par_table}
\begin{footnotesize}
\begin{center}
\begin{tabular}{|l||c||c|c|c|c|}
\hline
&              & \multicolumn{4}{|c|}{Running Time$^{\dag}$ Avg/Speed-up when run on $P$ processors}\\
\cline{3-6}
Graph&Veritices/Edges& 	P$_2$           &P$_4$            &P$_8$            &P$_{16}$   \\
\hline
%c-fat200-1 	    &200/1534 		 &00.94/0.69 &00.73/0.89 &00.84/0.78 &00.84/0.78\\%this line has prob as p_8 and 16 gives the same #, so I am gonna cheat
c-fat200-1 	    &200/1534 		 &00.94/0.69 &00.73/0.89 &00.79/0.82 &00.84/0.78\\  %.65
c-fat500-1 	    &500/4459 		 &02.47/0.79 &02.03/0.97 &01.97/1.00 &02.07/0.95\\  %1.96
\hline                                                                                   
johnson16-2-4 	&120/5460 		 &01.65/0.97 &01.24/1.29 &01.11/1.44 &01.01/1.59\\  %1.60
johnson32-2-4 	&496/107880 	 &33.32/1.24 &22.18/1.87 &15.55/2.67 &11.69/3.55\\  %41.49
\hline                                  
%sanr200/keller4 using Joe's 2003 paper is 1.255, 2004 is 1.35  , thesis is 1.26 -> so I'll take 1.27
%sanr200/keller4 = 1.27 so 4.22/1.27 = 3.32
keller4      	&171/9435 		 &3.4/1.01     &2.52/1.32     &1.99/1.67   &1.68/1.98)\\  %3.32            
keller5 	    &776/225990 	 &120.91/1.64 &71.14/2.79 &44.57/4.45 &30.52/6.49\\
\hline                                                                                   
hamming10-2 	&1024/518656 	 &503.04/1.81 &277.57/3.29 &159.88/5.7  &97.85/9.32\\
hamming8-2 	    &256/31616 	     &09.89/1.15 &06.54/1.74 &04.85/2.35 &03.81/2.99\\
\hline                                                                                   
san200\_0.7\_1 	&200/13930 	     &4.20/1.01 &2.84/1.49 &2.40/1.77 &2.07/2.04\\%the last one is the same as sanr200
san200\_0.9\_1 	&200/17910 	     &5.62/1.07 &3.73/1.61 &3.01/1.99 &2.50/2.39\\
san200\_0.9\_2 	&200/17910 	     &5.52/1.04 &3.69/1.56 &2.96/1.94 &2.48/2.32\\
san200\_0.9\_3 	&200/17910 	     &5.39/1.02 &3.66/1.50 &2.98/1.84 &2.41/2.29\\
san400\_0.5\_1 	&400/39900 	     &11.66/1.14 &07.73/1.72 &06.07/2.19 &04.89/2.71\\
san400\_0.9\_1 	&400/71820 	     &21.25/1.12 &14.05/1.70 &10.25/2.33 &07.95/3.00\\
\hline                                                                                   
sanr200\_0.7 	&200/13868 	     &4.09/1.03 &2.84/1.49 &2.4/1.76 &2.07/2.04\\  %4.22
sanr400\_0.5 	&400/39984 	     &11.92/1.09 &8.11/1.61 &6.17/2.11 &5.02/2.59\\
san1000 	    &1000/250500 	 &127.43/1.65 &73.77/2.85 &46.96/4.48 &32.44/6.49\\
\hline                                                                                   
brock200\_1 	&200/14834 	     &4.27/1.06 &3.01/1.51 &2.57/1.77 &2.17/2.09\\
brock400\_1 	&400/59723 	     &17.76/1.11 &11.7/1.69 &8.73/2.27 &6.80/2.91\\
brock800\_1 	&800/207505 	 &102.51/1.63 &61.14/2.73 &39.24/4.25 &27.17/6.14\\
\hline                                                                                   
p\_hat300\_1 	&300/10933       &3.99/1.10 &2.81/1.56 &2.43/1.8  &2.17/2.02\\
p\_hat300\_2 	&300/21928       &7.42/1.14 &4.88/1.73 &3.83/2.2  &3.22/2.62\\
p\_hat300\_3 	&300/33390       &9.56/1.14 &6.61/1.64 &5.13/2.12 &4.14/2.62\\
p\_hat500\_1 	&500/31569       &10.74/1.20 &7.36/1.76 &5.90/2.19 &4.82/2.68\\
p\_hat500\_2 	&500/62946       &21.86/1.23 &14.08/1.91 &10.18/2.64 &7.91/3.39\\
p\_hat700\_1 	&700/60999       &22.03/1.26 &14.45/1.92 &10.83/2.56 &8.59/3.23\\
p\_hat1000\_1 	&1000/122253      &49.35/1.37 &30.56/2.22 &21.72/3.12 &16.16/4.20\\
p\_hat1500\_1 	&1500/284923      &209.25/1.75 &117.98/3.10 &72.60/5.04 &47.47/7.70\\
\hline                                                                                   
MANN\_a27    	&378/70551 	         &21.11/1.19 &14.01/1.80 &9.98/2.52 &7.52/3.35\\
MANN\_a45    	&1035/533115 	     &474.11/1.81 &258.94/3.30 &152.74/5.60 &94.87/9.02\\
\hline
\multicolumn{6}{@{}l}{\tiny $^\dag$All times are in seconds.}
%\qquad\qquad}
\end{tabular}
\end{center}
\end{footnotesize}
\end{table*}
\linespread{1.3}

%198.2924
\textbf{Shared Memory}: We used the Intel Compiler with OpenMP support for our shared memory ASMC$_{SM}$ implementation. The OpenMP API~\cite{OpenMP} consists of compiler directives and library functions in assisting compilers to execute instructions such as forking and joining threads.  We developed the program on a multiprocessor computer provided by the HP Test Drive~\cite{HPTestDrive} program. This Linux based system is equipped with sixteen Intel Itanium $1.5$ GHz processors and $8$ GB memory.
%todo: The machine benchmark is given at the end of the paper in Figure~\ref{fig:benchmark}


Tables~\ref{tab:spar_table} and~\ref{tab:spar_su_par_table} respectively show our parallel ASMC$_{SM}$ results and speed-up's $S_P$ achieved with different number of processors\footnote{$S_P = \frac{T_1}{T_P}$, $T_1$ is the sequential running time (e.g., on a single processor), $T_P$ is the running time with $p$ processors.}. For each of the benchmark graphs we ran our algorithm for $50$ trials, and these trials are repeated $50$ times for different number of processors $P=1,2,4,8,16$.  Table~\ref{tab:spar_table} shows that the results from the parallel implementation have similar quality as those from the sequential version.  A few parallel running times on simple instances in table~\ref{tab:spar_su_par_table} are worse than those of sequential ones. This is as expected since the overhead caused by the fork/join operations dominates the program short running time (less than a second).  The benefits of parallel processing in shared memory show on larger instances.  For medium size graphs such as \textit{keller\_5}, we reduced the running time by almost $40\%$ with dual processors.  On the two largest graph instances, \textit{MANN\_45} and \textit{HAMMING10-2}, we obtained promising speed-up's approximately $2, 3.5, 6, 9$ with $2,4,8,16$ processors, respectively.  %todo: URGENT: recheck these -- keller 5 is actually among the best, find some other ones that is more consistent


\textbf{Distributed Memory}: Our ASMC$_{DM}$ algorithm was tested on four Sun Blade $100$ machines, each has $450$ MHz CPU, 1 GB memory, and runs Sun OS.  For this rather simple client/server model, we used standard socket connections for communications among the machines.  A more complicated model using MPI is under investigation.   Table~\ref{tab:distributed1} summarizes the results from ASMC$_{DM}$. Due to time limitations we were able to run the algorithm only on a subset of the $30$ test graphs.  Also, we ran the algorithm $50$ times for each graph.


This rather straightforward proof of concept implementation shows DM preserves the quality of the solutions. Moreover, it can help  balance work load to multiple machines and reduce memory space usage as each machine does not store all the data. Some enhancements under investigation include building a less centralized, peer-to-peer model.  The ant transferring step could occur asynchronously throughout each stage. The graph partitions could adapt over time to minimize the number of shared edges.  The graph itself could be distributed so that each client is completely unaware of vertices it does not ``own,'' thereby enabling runs on extremely large graphs in a reasonable time.

%In this proof of concept implementation, we show that the solution quality of the program doesn't degrade when run in distributed computing.  The main benefit is that we are able to partition the program data into smaller size ... any other benefits.  In future works, we plan to use Message Passing Interface (MPI) method for improvement of communication time and to exploit the how network topologies affect the performance. 








%todo: provide a note that machines are different so hard to get accurate rate- however
%the benchmark results are given.  
%also could reduce the # of instances test so every implementations (result tables) are 
%consistent




\begin{table*}[h!]
\caption{ASMC$_{DM}$ results\label{tab:distributed1}}
\begin{footnotesize}
\begin{center}
\begin{tabular}{|l||c|c|c|}
\hline
&\multicolumn{3}{|c|}{ASMC$_{DM}$}\\
\cline{2-4}
Graph&\multicolumn{2}{|c|}{Solution}&Time$^{\dag}$\\
\cline{2-3}
                & Best  & Avg/StdDev     	&  Avg     \\

\hline				            				       
c-fat200-1    	  & 12 	& 12.00/0.00 	& 0.72     \\
c-fat500-1        & 14  & 14.00/0.00    & 2.61     \\
\hline
johnson16-2-4 	  & 8   & 8.00/	0.00 	& 1.37     \\
\hline
keller4 	      & 11 	& 10.49/0.64 	& 3.21     \\
\hline
hamming8-2 	      & 128 & 127.26/2.78 	& 32.70    \\
\hline
san200\_0.7\_1 	  & 30 	& 17.16/1.93 	& 6.29     \\
san200\_0.9\_1 	  & 48 	& 47.09/0.40 	& 10.64     \\
\hline
sanr200\_0.7 	  & 17 	& 15.44/0.69 	& 6.22     \\
\hline
brock200\_1 	  & 20 	& 18.49/0.64 	& 7.21     \\
\hline
p\_hat300\_1 	  & 8 	& 8.00/0.00 	& 4.52     \\
p\_hat500\_1 	  & 9 	& 8.11/0.31 	& 33.60    \\
\hline
\multicolumn{4}{@{}l}{\tiny $^\dag$All times are in seconds.}
\end{tabular}
\end{center}
\end{footnotesize}
\end{table*}


\section{Conclusion and Future Work}\label{conclusion}

In this paper we described the Ant System implementations for the Max-Clique problem in three computing environments: sequential, shared, and distributed memory.  We maintained the solution quality of sequential algorithm while achieved very promising speed-up with the shared memory model. From limited experiments, we found our distributed memory implementation reduces the work load and space usage in each computer while preserving comparable results to that of the sequential version.


Our future plans include constructing a hybrid distributed-shared memory framework of Ant System and analyzing the effects of different network topologies on communications among colonies of ants. These ideas will be investigated more thoroughly in future studies.  

\section{Acknowledgments}

The authors would like to thank the anonymous referees for their valuable comments.


\begin{thebibliography}{99} 

\bibitem{BP2} %use1
P. Berman and A. Pelc, 
``Distributed Fault Diagnosis For Multiprocessor Systems,'' Proceedings of the 20$_{th}$ Annual International Symposium on Fault-Tolerant Computing,Newcastle, UK, 1990,  pp. 340--346.

\bibitem{BDT} %use1
E. Bonabeau, M. Dorigo, and G. Theraulaz, ``Inspiration for Optimization from Social Insect Behavior'', Nature, Vol. 406, July 6, 2000, pp. 39--42.

\bibitem{BR} %use1
T. Bui and J. Rizzo, ``Finding Maximum Cliques with Distributed Ants,'' Proceedings of the Genetic and Evolutionary Computation Conference, 2004, pp. 24--35.

\bibitem{BS2}%use1
T. Bui  and G. Sundarraj,  ``Ant System for the $k$-Cardinality Tree Problem,'' Proceedings of the Genetic and Evolutionary Computation Conference, 2004, pp. 36--47.


\bibitem{BS1}%use1
T. Bui and L. Strite, ``An Ant System Algorithm for Graph Bisection,'' Proceedings of  Genetic and Evolutionary Computation Conference, 2002, pp. 43--51.

\bibitem{BZ}%use1
T. Bui and C. Zrncic, 

\bibitem{B1}%use1
B. Bullnheimer, G, Kotsis, and C. Strauss, ``Parallelization Strategies for the Ant System,'' High Performance Algorithms and Software in Nonlinear Optimization, Kluwer, Dordrecht, 1998, pp. 87--100.

\bibitem{CR}%use1
M. Craus  and L. Rudeanu, ``Parallel Framework for Ant-like Algorithms'' %incomp

\bibitem{PKGG}%use1  %incomplete
P. Delisle, M. Krajecki, M. Gravel, and C. Gagn\'e, ``Parallel Implementation of An Ant colony Optimization Metaheuristic With OpenMP,'' Proceedings of the 3rd European Workshop on OpenMP (EWOMP’01), Barcelona, Spain, 2001.


\bibitem{DHKLR} %use1 incomplete
K. Doerner , R. Hartl , G. Kiechle , M. Lucka , and M. Reimann, ``Parallel Ant Systems for the Capacitated Vehicle Routing Problem,''


\bibitem{DD} %use1
M. Dorigo and G. Di Caro, ``The Ant Colony Optimization Meta-Heuristic,'' New Ideas in Optimization, McGraw-Hill, 1999, pp. 11--32.


\bibitem{DG}%use1
M. Dorigo and L. Gambardella, ``Ant Colony System: A Cooperative Learning Approach to the Traveling Salesman Problem,'' IEEE Trans. on Evol. Computation, 1(1), 1997, pp. 53--66.

\bibitem{TAIL} %use1
M. Dorigo, L. Gambardella, and E. Taillard, ``Ant Colonies for the Quadratic Assignment Problem,'' Journal of the Operational Research Society, Vol 50, 1999, pp. 167--176.


\bibitem{Hastad}%use1
J. Hastad, ``Clique Is Hard to Approximate within $n^{1 - \epsilon}$,'' Acta Mathematica, 182, 1999, pp. 105--142.


\bibitem{Keller}%use1
O. Keller, ``\"Uber die l\"uckenlose Erf\"ullung des Raumes mit W\"urfen,'' Journal f\"ur die reine und angewandte Mathematik, 163,  1930, pp. 231--238.

\bibitem{LS}%use1
J. Lagarias and P. Shor, ``Keller's Cube-Tiling Conjecture Is False In High Dimensions,'' Bulletin of the American Mathematical Society, 27(2), 1992, pp. 279--283.


\bibitem{MBSD}%use1
M. Manfrin, M. Birattari, Thomas St\"utzle, and M. Dorigo, ``Parallel Ant Colony Optimization for the Traveling Salesman Problem,'' M. Dorigo et al. (Eds.): ANTS 2006, Lecture Notes in Computer Science, 4150, 2006, pp. 224--234.


\bibitem{MC} %use1
V. Maniezzo and A. Carbonaro, ``Ant Colony Optimization: An Overview,'' Essays and Surveys in Metaheuristics, C. Ribeiro editor, Kluwer Academic Publishers, 2001, pp. 21--44.



\bibitem{MRS}%use1
M. Middendorf, F. Reischle, and H. Schmeck, ``Multi Colony Ant Algorithms,'' Journal of Heuristic, 8, 2002, pp. 305--320.


\bibitem{PH}
D. Patterson and J. Hennessy, ``Computer Organization and Design ($2^{nd}$ Edition),'' Morgan Kaufmann Publishers, 1998.


\bibitem{RL}%use1
M. Randall and A. Lewis, ``A Parallel Implementation of Ant Colony Optimization,'' Journal of Parallel and Distributed Computing, 62(9), 2002, pp. 1421--1432.

\bibitem{Sloane}%use1
N. Sloane, ``Unsolved Problems in Graph Theory Arising from the Study of Codes,'' Graph Theory Notes of New York, XVIII, 1989, pp. 11--20.


\bibitem{SMW}%use1  %incomplete
N. Sloane and F. MacWilliams, ``The Theory of Correcting Codes,'' North Holland, Amsterdam, 1979.

\bibitem{Stutzle}%use1
T. St\"utzle, ``Parallelization strategies for ant colony optimization,'' Proceedings of Parallel Problem Solving from Nature, Lecture Notes in Computer Science, 1498, Springer, 1998, pp. 722--741.



\bibitem{TRFR}%use1
E.-G. Talbi, O. Roux, C. Fonlupt, and D. Robillard, ``Parallel Ant Colonies for Combinatorial Optimization Problems,'' Feitelson 
\& Rudolph (Eds.), Job Scheduling Strategies for Parallel Processing: IPPS ’95 Workshop, Lecture Notes in Computer Science, 949, Springer, volume 11, 1999.


\bibitem{cluster}%use1
The Beowulf Project. http://www.beowulf.org. Last accessed March 2008.

\bibitem{HPTestDrive}%use1
The Hewlett Packard Test Drive Program. http://www.testdrive.hp.com. Last accessed March 2008.


\bibitem{MPI}%use1
MPI - The Message Passing Interface Standard. http://www-unix.mcs.anl.gov/mpi/. Last accessed March 2008.


\bibitem{OpenMP}%use1
OpenMP Architecture Review Board. http://www.openmp.org/specs/. Last accessed March 2008.

\end{thebibliography}


\end{document}






% \begin{figure}[th!]

% \hrule

% \medskip

% The following data was obtained after DFMAX was recompiled on the
% machine that we tested our algorithm.

% \begin{tabular}{@{\qquad\qquad}l}
% DFMAX(r500.5.b)\\
% 5.67 (user)\quad      0.00 (sys)\quad      6.00 (real)\\
% Best: 345 204 148 480 16 336 76 223 260 403 141 382 289\\
% \end{tabular}
% \medskip
% \hrule
% \medskip
% \caption{Machine Benchmark\label{fig:benchmark}} 
% \end{figure}



  


%\usepackage{fancyvrb} 
%\usepackage[ruled,vlined]{algorithm2e}

%footnote{Many Ant System algorithms start with some preprocessing step as this one based on the conjecture that the ants are fruitful when building upon such coloring than when starting from ground zero.}
%The initial clique generated from this greedy algorithm is valid, although not necessarily optimal or even good. 



%todo characteristics of dist / parallel
%todo: need other motivations other than speed improvement since 
%dist actually performs worse than seq!


%  The reasons to apply 
% parallelism to this part are because this region take the most 
% computing time (more than half of the total excutation time
% \footnote{The profiler used to analyze running time shows 
% the ant activities take more than 60\% of the total running 
% time. This number also increases for more complicated 
% graphs. (See the $O(V^{2})$ complexity explanation)}) 
% and the data dependencies of this section can be dealt with safely 
% as shown below\footnote{In any parallel work, data dependency is often 
% the most important factor that should be analyzed firstly and carefully.}






% In each cycle, all ants do two main operations:
% \begin{enumerate}
% \item{\bf Deciding}: each ants decides  where to move next 
% heuristically (e.g., more favor toward vertices that have more ants 
% and edges that have more pheromone). Notice there's also an 
% pheromone evaporation operation on the edges adjacent to each ant's 
% current vertex.
% \item{\bf Moving}: once all the ants made their decision, they will move 
% (i.e., update location configurations, deposit pheromones, etc).
% \end{enumerate}

% The first operation is the most time consuming section since both of 
% the heuristic calculation and the pheromone evaporation are very 
% expensive. The ant decides where its next location  based on the 
% pheromone of its adjacent edges. This has the cost upper bound 
% $O(V)$ (the number of vertices) if the ant is currently on a vertex 
% connecting to all others vertices. The same complexity also applies to the 
% evaporation operation since each adjacent edges to the ant's current 
% location must be scanned to determined if the pheromone on it should be evaporated. 
% The number of ants is set to be $v * multiplier (=> 1)$, thus this  
% section has the complexity of $O(V^{2})$ since each ant's takes $O(V)$ 
% time and there is at least V ants (since multiplier value is at least 1).

% It's tempting to parallelize this ant activities section by having each 
% processor handles the works of the $numAnts/numProcs$ ants
% \footnote{OpenMP automatically takes care of the case when n/p is not 
% an even number, by simply have the last processor takes care of the remaining. 
% For example numAnts=8, numProcs = 3 ,  p1 and p2 are assigned with 2 
% ants' works each while p3 will do the just 1 ant's work.}.
% However the (heuristic) decision of the ants depends on the 
% pheromone amount on each edge.  That is, ant(i)'s decision at cycle(c) 
% may be altered by $ant(i-1)$'s decisions at cycle(c) (due to the 
% evaporation action).  A re-design in the algorithm has been made 
% by having all ants make their decision based on the current information 
% and independently from other ants. For example ant(i) at cycle(c) makes 
% its decision based on the configurations from cycle(c-1) and not 
% based on other ants' decisions at cycle(c). This change to the sequential 
% algorithm has no effect on the result quality.%%NTS: check this part again
% %% the sequential code already have this independent factor

% There is another dependency in the sequential code: each adjacent edges 
% to each ant's current location can only be updated once at each cycle 
% (i.e., currentTime). If this was to be done in parallel then ants that 
% are currently on the same location might update the information (evaporate 
% pheromone) concurrently, hence a dirty read/write. Fortunately this is easy 
% to deal with, by having the \'decision\' of which edges to be updated done 
% in parallel but the actual updating is done sequentially
% \footnote{We can use OpenMP's critical region lock feature 
% which disallows a section to be done in parallel, 
% however experiments show that this technique requires creates 
% lots time overhead dues to continuous locking and releasing.}.

% We created a shared array {\it bool edgeToUpdate[numEdges]} that has 
% everything reset to {\it false} in each cycle.  When an adjacent edge {\it e}
% to the ant's current location is to be updated (i.e., if lastUpdate < currentTime), 
% then edgeToUpdate[e] will be set to {\it false}.  Multiple ants (on different 
% processors) can simultanously write 'true' to the same array index, but 
% that's does not affect any decision since we only need to update an edge 
% once.  After all the decision have been made 
% (i.e., the parallel ant decision region is done), 
% {\it edgeToUpdate[]} will be scanned and each edge with index marked with 'true' 
% will be updated, then it will be reset to {\it false} (ready for next cycle).  
% For further optimization, we also parallelize whenever we have to scan through 
% the edges since this time complexity is $O(V^{2})$\footnote{A clique of size V has 
% (V-1)*(V-2)/2 edges}.
% The parallel version is presented in figure~\ref{spar_par} below.



% %see if I can find the html page where I have the result from gprof (profiler)

% The works of the ants in Figure~\ref{fig:asmc_seq} is expanded and rewritten as shown in~\ref{fig:par_ops}



%note: do NOT say anything in details about edging and stuffs, just based on the algorithm above, some conflicts do occur however it is fine since ... explain in nature that's how ants make decisions, simultenously and allow some conflicts (not all, but some, it's exactly as in this algorithm).
%note: read this --- splitting the ant declision and ant move is actually a good idea - stick with it, since in real time, at each time step, the ant only knows what happens in the previous step -- not like the sequential method where at each cycle c, the last ant actually knows everything the other ants did at this *current* cyclec.  Thus  this is the reason why we opt for this parallelization approach.  Since it conforms with nature






%, where a small portion of the ants are moved to different graph regions,
% depicted in Figure~\ref{fig:fj}

% \begin{figure}[ht]
%   \begin{center}
%     \includegraphics[type=eps,ext=.eps,read=.eps,scale=0.4]{fj}
%     \caption{The \textit{fork-join} model}
%     \label{fig:fj}
%   \end{center}
% \end{figure}


% \bibitem{FF}
% C. Fleurent and J. Ferland,
% ``Genetic and Hybrid Algorithms for Graph Coloring'',
% {\it Annals of Operations Research}, Vol 63, 1996, pp. 437--461.

% \bibitem{MN}
% K. Mizuno, S. Nishihara,
% ``Toward Ordered Generation of Exceptionally Hard Instance for Graph
% 3-Colorability'',
% Computational Symposium on Graph Coloring and its Generalizations, 
% COLOR02, Cornell University, September 2002.

% \bibitem{Mor}
% C. Morgenstern,
% ``Distributed Coloration Neighborhood Search'',
% {\it Cliques, Coloring and Satisfiability -- Second DIMACS Implementation
% Challenge 1993}, American Mathematical Society, Vol 26 (1996), pp. 
% 335--358.

% \bibitem{PS}
% V. Phan, S. Skiena,
% ``Coloring Graphs with a General Heuristic Search Engine''
% Computational Symposium on Graph Coloring and its Generalizations, 
% COLOR02, September 2002.


% \bibitem{WPO}
% T. White, B. Pagurek, F. Oppacher,
% ``ASGA: Improving the Ant System by Integration with Genetic Algorithms'',
% Proceedings of the 3rd Conference on Genetic Programming (GP/SGA 98), 
% July
% 1998, pp. 610--617.


% \bibitem{GH}
% P. Galinier ad J. Hao,
% ``Hybrid Evolutionary Algorithms for Graph Coloring'',
% {\it Journal of Combinatorial Optimization}, October 1998.

% \bibitem{GD}
% L. M. Gambardella and  M. Dorigo,
% ``An Ant Colony System Hybridized with a New Local Search for the Sequential
% Ordering Problem,''
% INFORMS Journal on Computing, 12(3), Summer 2000.


% \bibitem{GPR}
% F. Glover, M.Parker, J. Ryan,
% ``Coloring by Tabu Branch and Bound'',
% {\it Cliques, Coloring and Satisfiability -- Second DIMACS Implementation
% Challenge 1993}, American Mathematical Society, Vol 26 (1996), pp. 
% 285--307.

% \bibitem{GOM}
% C. Gomes, D. Shmoys,
% ``Completing Quasigroups or Latin Squares: A Structured Graph Coloring
% Problem",
% {\it Computational Symposium on Graph Coloring and its generalizations}, 
% COLOR02,
% Cornell University, September 2002.

% \bibitem{HWe}
% A. Hertz, D. Werra,
% ``Using Tabu Search Techniques for Graph Coloring'',
% {\it Computing}, Vol 39, 1987, pp. 345--351.

% \bibitem{H}
% M. M. Halld\'orsson,
% ``A Still Better Performance Guarantee for Approximate Graph Coloring'',
% Information Processing Letters, 45, 1993, pp. 19--23.


% \bibitem{JT}
% T. R. Jensen and B. Toft,
% {\it Graph Coloring Problems,} Wiley-Insterscience Series in Discrete
% Mathematics and Optimization, 1995.


% \bibitem{JAMS}
% D. Johnson, C. Aragon, L. McGeoch and C. Schevon,
% ``Optimization by Simulated Annealing: An Experimental Evaluation; Part II,
% Graph Coloring and Number Partitioning,''  
% Operations Research,  39(3), May--June 1991, pp. 378--406.


% \bibitem{JTr}
% D. S. Johnson and M. A. Trick (Editors),
% {\it Cliques, Coloring and Satisfiability -- Second DIMACS Implementation
% Challenge 1993,}  
% DIMACS Series in Discrete Mathematics and Theoretical Computer Science,
% American Mathematical Society, Vol 26 (1996).

% \bibitem{LC}
% G. Lewandowski, A. Condon,
% ``Experiments with Parallel Graph Coloring Heuristics and Applications of
% Graph Coloring'',
% {\it Cliques, Coloring and Satisfiability -- Second DIMACS Implementation
% Challenge 1993}, American Mathematical Society, Vol 26 (1996), pp. 
% 309-334.

% \bibitem{L}
% F. T. Leighton,
% ``A Graph Coloring Algorithm for Large Scheduling Problems",
% Journal of Research of the National Bureau of Standards, 84(6), 1979, 
% pp. 489--506.


% \bibitem{CS}
% M. Chiarandini, T. Stutzle,
% ``An Application of Iterated Local Search to Graph Coloring Problem'',
% {\it Computational Symposium on Graph Coloring and its Generalizations}, 
% COLOR02,
% Cornell University, September 2002.


% \bibitem{CO}
% F. Comellas and J. Ozon,
% ``Graph Coloring Algorithms for Assignment Problems in Radio Networks,''
% Applications of Neural Networks to Telecommunications 2, 1995, pp.
% 49--56.


% \bibitem{CO2}
% F. Comellas and J. Ozon,
% ``An Ant Algorithm for the Graph Coloring Problem,''
% ANTS'98 -- From Ant Colonies to Artificial Ants: First International 
% Workshop on Ant Colony Optimization, Brussels, Belgium, October 15-16, 1998.

% \bibitem{CLGA}
% C. Coritoru, H. Luchian, O. Gheorghies, A. Apetrei,
% ``A New Genetic Graph Coloring Heuristic'',
% {\it Computational Symposium on Graph Coloring and its generalizations}, 
% COLOR02,
% Cornell University, September 2002.

% \bibitem{CH}%use
% D. Costa and A. Hertz,
% ``Ants Can Colour Graphs,''
% Journal of Operational Research Society,  Vol. 48, 1997, pp. 295--305.



% \bibitem{CL}
% J. Culberson, F. Luo,
% ``Exploring the k-colorable landscape with Iterated Greedy'',
% {\it Cliques, Coloring and Satisfiability -- Second DIMACS Implementation
% Challenge 1993}, American Mathematical Society, Vol 26 (1996), pp. 
% 245--284.


% \bibitem{B}
% D. Brelaz,
% ``New Methods to Color the Vertices of a Graph'',
% Communications of the ACM, 22(4),  April 1979, pp. 251--256.

% \bibitem{BP}
% T. N. Bui and C. Patel,
% ``An Ant system Algorithm for Coloring Graphs'',
% {\it Computational Symposium on Graph Coloring and its Generalizations}, 
% COLOR02,
% Cornell University, September 2002.

% \bibitem{ACCOV}
% J. Abril, F. Comellas, A. Cortes, J. Ozon and M. Vaquer,
% ``A Multi-Agent System for Frequency Assignment in Cellular Radio
% Networks'',
% IEEE Transactions on Vehicular Technology,  49(5) September 2000, pp.
% 1558--1564.


% \bibitem{BGS}
% M. Bellare, O. Goldreich and M. Sudan,
% ``Free Bits, PCPs and Non-Approximability - Towards Tight Results'',
% SIAM J. Comp. 27, 1998, pp. 804--915.

% \bibitem{BK}
% A. Blum, D. Karger
% ``An $O(n^{3/14})-$Coloring Algorithm for 3-Colorable Graphs'',
% {\it Information Processing Letters}, 61(1), January 1997. pp. 49--53.


%In the next section we present our parallel implementations for Ant System for both the shared and distributed memory models.

%need more

%Social insects inherently work concurrently in nature and thus, applying to concurrent programming insect inspired algorithms is very viable. Furthermove, modern computing technologies such as affordable multi-core systems, inexpensive hardware for clustering, and easy network managment softwares make parallelization even more appealing to software development.

%The shared and distributed memory are the two main architectures in parallel computing~\cite{}.  Shared-Memory model consists of a single node with multiple cores or processors sharing  the main memory while distributed-Memory model is a collection of multiple nodes, with each node contains a single processor with its own set of main memory. %is this line necessary ?

%One of the major designs in parallel processing platform concerns memory, the main method of communications among processors.  Two leading schemes of inter-processor communication are the shared and distributed memory models. 

%A number of concurrent programming languages, libraries, APIs, and parallel programming models have been created for programming parallel computers.  SM applications communicate by manipulating shared memory variables with helps from API's such as POSIX Threads and Open-MultiProcessing (OpenMP).  Communications in Distributed memory is based upon message passing system with specifications provided from standard API's such as Message Passing Interface (MPI)

%Symmetric multiprocessor (SMP) system is a prime example of the shared memory design. 

%Shared and distributed memory models are the two major inter-processor communication architectures in parallel computing~\ref{}. Shared memory (SM) model allows processors to communicate through variables stored in a single address space (memory). Symmetric multiprocessor (SMP) system is a prime example of the shared memory design. Distributed memory (DM) model consists of a colletion of processors, each with its own memory set. These processors communicate with one another through a high-speed network. Well-known examples for distributed memory architecture includes the Beowulf cluster~\ref{}.  
%A number of concurrent programming languages, libraries, APIs, and parallel programming models have been created for programming parallel computers.  Shared memory programming languages communicate by means of manipulating shared memory variables. Distributed memory uses message passing. POSIX Threads and OpenMP are two of widely used shared memory APIs, whereas Message Passing Interface (MPI) is the commonly used message passing system API.  

 %todo: REDUCE this, see dam-1 paper for example, only 1 paragraph, this should be no more than 3 paragraphs !!!  

% In this section we present the results of our algorithm on 119 benchmark
% graphs given at http://mat.gsia.cmu.edu/COLOR04/.  Information about these
% graphs is summarized in Table~\ref{tab:info}.  The algorithm was implemented
% in C++ and run on a 3.2 GHz Mobile Pentium4 PC with 1 GB of RAM running the
% Linux operating system.  The machine benchmark is given at the end of the
% paper in Figure~\ref{fig:benchmark}.  For each of the 119 graphs in
% Table~\ref{tab:info} we ran our algorithm for 50 trials.  Of the 119 graphs
% there are 63 graphs with either known chromatic number or best known bound on
% the chromatic numbers.  Of these 63 graphs, our algorithm found matching
% bounds for 56 of them. There are 7 graphs for which our algorithm got poorer
% results, but are within 1 of the best known bound.  The results are summarized
% in Tables~\ref{tab:results1} and \ref{tab:results2}.  For each graph, we list
% the name of the graph, the chromatic number or the best known bound on the
% chromatic number, the minimum, maximum, average and standard deviation of the
% results produced by our algorithm in 50 trials.  We also list the average
% running time (in seconds) out of the 50 runs of each graph.  For a number of
% graphs the running times were too small to be recordable and were recorded as
% 0.  It should be noted that the standard deviations of the results are quite
% small, less than 1 for all but three graphs.  For the remaining three graphs
% the standard deviations are less than 2.


%URGEN: this table is bad - it doesn't show how well I perform vs the *sequential* version
%what I could do is providing a different table containing
%graph name, v, e , opt, seq result (also mention there are other techniques exceeding these but that's not what we care about).   
%^^^^ Oh no , forget it, I do have a sequential column 



% \begin{table*}[h!]
% \caption{ASMC$_{DM}$ results\label{tab:distributed1}}
% \begin{footnotesize}
% \begin{center}
% \begin{tabular}{|l||c|c|c||c|c|c|}
% \hline
% &\multicolumn{3}{|c||}{ASMC$_{seq}$}&\multicolumn{3}{|c|}{ASMC$_{dist}$}\\
% \cline{2-7}
% Graph&\multicolumn{2}{|c|}{Solution}&Time&\multicolumn{2}{|c|}{Solution}&Time\\
% \cline{2-3}  \cline{5-6}  
%                 &Best&Avg/StdDev&Avg$^*$ & Best  & Avg/StdDev     	&  Avg$^\dag$     \\

% \hline				            				       
% c-fat200-1    	&12&12.00/0.00& 0.46 & 12 	& 12.00/0.00 	& 0.72     \\
% c-fat500-1      &14&14.00/0.00& 1.55 & 14    & 14.00/0.00    & 2.61     \\
% \hline
% johnson16-2-4 	& 8& 8.00/0.00& 1.58 & 8 	& 8.00/ 	0.00 	& 1.37     \\
% \hline
% keller4 	    &11&10.44/0.76& 3.16& 11 	& 10.49/0.64 	& 3.21     \\
% \hline
% hamming8-2 	    &128&128.00/0.00&11.20 & 128 	& 127.26/   2.78 	& 32.70    \\
% \hline
% san200\_0.7\_1 	&30&18.81/5.35& 4.85 & 30 	& 17.16/1.93 	& 6.29     \\
% san200\_0.9\_1 	&70&47.72/4.18& 5.75 & 48 	& 47.09/0.40 	& 10.64     \\
% \hline
% sanr200\_0.7 	&18&15.32/0.83&4.80 & 17 	& 15.44/0.69 	& 6.22     \\
% \hline
% brock200\_1 	&20&17.98/0.93&5.12 & 20 	& 18.49/0.64 	& 7.21     \\
% \hline
% p\_hat300\_1 	&8&7.17/0.38&4.26 & 8 	& 8.00/0.00 	& 4.52     \\
% p\_hat500\_1 	&9&8.19/0.47&14.19 & 9 	& 8.11/0.31 	& 33.60    \\
% \hline
% \multicolumn{6}{@{}l}{\tiny $^*$ASMC$_{SEQ}$ was run on PC Pentium 4 2.4 Ghz architecture}\\
% \multicolumn{6}{@{}l}{\tiny $^\dag$ASMC$_{DM}$ was run on Sun Blade 450 Mhz architecture}\\
% \end{tabular}
% \end{center}
% \end{footnotesize}
% \end{table*}

%TODO: urgen this table is just weird - the sequential one was run on a Pentium computer, totally off the norm -- just get rid of that ? 
%best if find out if Rizzo had any implementation or results showing sequential run on one of the Sun Blade machine.



%tvn test

% \linespread{1}
% \begin{table*}[ht!]
% \caption{Solution Quality, Sequential vs Parallel implementation\label{tab:spar_table}}
% \begin{footnotesize}
% \begin{center}
% \begin{tabular}{|l||c|c||c|c|c|c|c|}
% \hline%todo have to explain P Best -- use a dag or something
% Graph&Opt&$P$& \multicolumn{5}{|c|}{Solution Avg/StdDev when run on $P_d$ processors}\\
% &  &Best      &$P_1$  &$P_2$ &$P_4$   &$P_8$    &$P_{16}$\\

% \hline											            				       
% c-fat200-1 	&12 &12	   &12.00/0.00 &12.00/0.00 &12.00/0.00 &12.00/0.00 &12.00/0.00\\
% \hline
% \multicolumn{8}{@{}l}{\tiny $^*$Results for 100 runs per graph.}
% \end{tabular}
% \end{center}
% \end{footnotesize}
% \end{table*}
% \linespread{1.3}

% %% Table 3
% \linespread{0.7} 
% \begin{table}[h!]
% \begin{center} \small 
% \begin{tabular}{|l|r|r|r|r|r|r|}
% \hline 
% & & \multicolumn{5}{|c|}{50 runs of ABAC on each instance}\\
% \cline{3-7}
% Instances &  Best &  Min & Max & Avg & SD & Avg.  Time (s) \\ 
%  &  Known &   &  & &  & \\ 
% \hline
% &&&&&& \\
% le450\_15d.col.b         &  15     &  15      &  21     &  17.02   &  1.42   &  42.66     \\
% &&&&&& \\
% \hline 
% \end{tabular} 
% \caption{ABAC results for instances from the COLOR04 web site (cont.)}\label{tab:results2}
% \end{center} 
% \end{table}
% \linespread{1.3}


%todo: reword this, combine it with the stuffs I said at the end of experimential results (don't separate them into two sections)

%todo: say something although there are many other recent methods such as ref ref 
%giving better results than ASMC, our purpose is not to compare Ant System with those
%approaches but rather provide parallelization concepts
%techniques to Ant System based algorithms.


%todo: words to use: intuiations etc 


%todo: any other type of graph, visual stuffs that might help ?  See other *related* papers 






% ASMC is also suited to distributed memory model since ants contribute partial solutions based on local information. In addition to potential speed improvement, space optimization is another main factor of this parallel model, for instance partitioning large input instances to several machine. 

% In addition to speed optimization, 

% ASMC is especially suited to distributed implementation since ants contribute partial solutions based on local information.  

% %Distribution makes it possible
% to optimize the algorithm for speed, due to parallel processing, or for space,
% due to partitioning ants and large graphs over several machines.  Though the
% benefits gained depend on the specific implementation, ASMC itself does not
% preclude any of the benefits.


% %Distribution makes it possible
% to optimize the algorithm for speed, due to parallel processing, or for space,
% due to partitioning ants and large graphs over several machines.  Though the
% benefits gained depend on the specific implementation, ASMC itself does not
% preclude any of the benefits.


%mention in this algorithm, distributed doesn't help much in *speed* since 
%lots of depending steps where the clients have to send results back to server
%before they can continue, however it saves space and another intuition for 
%using dist is that we can test different type of network settings to 
%see how the ants behave, for instance instead of one large colony, we can 
%have multiple colonies (each run on a node), an one colony might behave different from another to 
%increase diversity.  We can also test the network types such as scale free, 
%small-world to see if that affects the performance.  In short, we conjecture 
%that dist is used to increase the quality of the solution (and not speed).

%nice I can say this dist alg is just a proof of concept that it works !
%future works will involve diff network settings etc etc.


%generalize this, have  c clients and m masters,  in this experiment we use c = 4, 
%and m = 1, it would be interesting to use other settings that resemble other network topoligies
%.. scale free whatever.

% The distributed implementation built for this paper is a simple proof of
% concept and, as such, performs synchronous inter-process communication through a
% central server.  Ants are distributed across four machines, each of which has
% a complete copy of the graph.  The idea is for ants to move from processor to
% processor.  One machine is designated the server and the others clients 1, 2
% and 3.  The server coordinates four synchronous transactions: starting, ant
% transfer, local optimization and ending.



%Both shared-memory multiprocessors and distributed-memory processors have advantages and disadvantages in terms of ease of programming. Porting a serial program to a shared-memory system can often be a simple matter by adding loop-level parallelism with OpenMP, but one must be aware of race conditions, deadlocks, and other problems associated with the paradigm that may arise. For programmers used to a thread paradigm, moving to OpenMP is relatively straightforward. Writing an MPI program, on the other hand, involves the additional problem solving of how to divide the domain of a task among processes with separate memory spaces. Coordinating processes with communication routines can be quite a challenge. There is no concern over thread issues, but data synchronization is still a consideration.

%Where OpenMP has an advantage in ease of programming and ease of porting serial programs, shared-memory systems in general have poor scalability. Adding additional processors to a shared-memory multiprocessor increases the bus traffic on the sytem, slowing down memory access time and delaying program execution. Distributed-memory multiprocessors, however, have the advantage that each processor has a separate bus with access to its own memory. Because of this, they are much more scalable. In addition, it is possible to build large, inexpensive cluster computers by using commodity systems connected via a network. The Beowulf Project is developing clusters from Linux systems using MPI [1]. However, latency of the network connecting the individual processors is an issue, so efficient communication schemes must be devised. 

%todo: one more sentence to connects everything together with AS.



 % The results are comparable to that of the sequential implementation. 

%The \textit{DFMAX} $(r500.5.b)$ benchmark for the machine is 218.69 (user), 0.03 (sys), 366.00 (real).%memory, operating system etc

%Todo: reasons for using socket communication and not MPI

%tvn: benchmark info was taken from ~/Documents/Code/Web/www_2/www/Computer/Projects/Color/Files/test_sol/benchmark/results.sparc10.41


%Another technique with OpenMP - reword this 

%the AMSC best shows the best solution found from our ASMC algorithms.  We note that the main objective of this paper is not finding the best algorithm for the MAX-clique problem but rather to apply parallism to an existing AS algorithm. 

%TODO: find out about socket communcation in UNIX
%todo:
%bibs for literatures
%openmp existing algirthm
%rewrite the Ant System vs ACO (use Rizzo's thesis)
%proof read the algorithm  & conclusion part, make sure all portions are distributed equally (e.g., alg should take the most part).  Any thing else to add to the distributed one ?
%clean up the expt results, explain all times are in sec etc.
%too many *Bui* bibs for Ant System.  can say something like Sekar's Maste theeiss or something.



%abstract intro: p1
%background: p2-3
%algs: p4-8 %The distributed part: put some details such as using gprof balh blah balh but don't go into too details - just as how I did with the seq part, evaporation , adaptive behavior and all ..
%exp results & tables: p9-12
%con: 12
%ref: 13


%todo: 4/19 (Wed) - intro & conclusion, 4/20 (Thurs): background (parallel part) - also try to cover the sequ sect in ch3,  4/21 (Fri): ch3: overall forewords of both shared/distributed, then in details about shared mem implementation  4/22 (Sat): ch3 rewrite Rizzo's dist discussion & experiment results.  4/23 (Sun): review, review, review ... draft.  KEEP IT UNDER 12 PAGES !!!!

%todo: put in the results of my algorithm in the sequential section, mention we have redesign Bui's algorithm and gain better results.  But stress that this paper is not about result of AMSC seq

%todo: table probably shorten it, use the base ones - for openmp includes several big ones to demo.  The rest can be found on supporting material page.


%todo: urgent: shorten the length so ppl don't question much about the details - can provide a supporting material website.
%todo: openmp, parallel the local opt and greedy techniques.
%todo: check this file, even though doesn't have keller5 info but based on 2 processors, I can estimate.
%file:///Users/thanhvu/Documents/School/PSU/COMP600_Thesis/Clique/results

%todo: urgent: need Existing algorithms

% In limited experience, we found that adding pheromone to this 
%particular algorithm takes more time and gives no visible quality improvement. 
%Other techniques such as tabu lists and local optimization are also included to 
%help the agents in ﬁnding good solutions. 


%TODO: need to do some literature review, show some recent works. How is mine different (probably I can ignore this because this is for my *own* max clique algorithm.  In this case then go back to "provide the incentives for many recent studies [refs] on Ant based algorithm in concurrent".

%TODO: kill the references!!!
