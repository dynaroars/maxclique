%todo: 4/19 (Wed) - intro & conclusion, 4/20 (Thurs): background (parallel part),  4/21 (Fri): overall forewords of both shared/distributed, then in details about shared mem implementation  4/22 (Sat): ch3 rewrite Rizzo's dist discussion & experiment results.  4/23 (Sun): review, review, review ... draft.  KEEP IT UNDER 10 PAGES !!!!

%todo: put in the results of my algorithm in the sequential section, mention we have redesign Bui's algorithm and gain better results.  But stress that this paper is not about result of AMSC seq

%todo: table probably shorten it, use the base ones - for openmp includes several big ones to demo.  The rest can be found on supporting material page.


%todo: urgent: shorten the length so ppl don't question much about the details - can provide a supporting material website.
%todo: openmp, parallel the local opt and greedy techniques.
%todo: check this file, even though doesn't have keller5 info but based on 2 processors, I can estimate.
%file:///Users/thanhvu/Documents/School/PSU/COMP600_Thesis/Clique/results

%todo: urgent: need Existing algorithms

% In limited experience, we found that adding pheromone to this 
%particular algorithm takes more time and gives no visible quality improvement. 
%Other techniques such as tabu lists and local optimization are also included to 
%help the agents in Ô¨Ånding good solutions. 


%TODO: need to do some literature review, show some recent works. How is mine different (probably I can ignore this because this is for my *own* max clique algorithm.  In this case then go back to "provide the incentives for many recent studies [refs] on Ant based algorithm in concurrent".

%TODO: kill the references!!!

\documentclass[11pt]{article} 
%\usepackage[dvips]{graphicx} 
%\usepackage[bf]{caption2} 
\usepackage{amsmath}
\usepackage[figure,ruled,vlined]{algorithm2e}
\setlength{\textwidth}{6in} 
\setlength{\textheight}{8.5in} 
\setlength{\oddsidemargin}{0.2in}
\setlength{\evensidemargin}{0.5in}
\setlength{\topskip}{0in} 
\setlength{\topmargin}{0in} 
\setlength{\headsep}{0in}




\newcommand{\myfont}[1]{{\usefont{OT1}{cmss}{m}{it}#1}}
  
\begin{document}  

\title{Implementing the Max-Clique Ant System Algorithm in Parallel Environments} 
\author{ 
ThanhVu Nguyen\\ 
Computer Science Program\\ 
Penn State Harrisburg\\ 
Middletown, PA 17057\\ 
}  
\maketitle  


\begin{abstract}
In this paper we present techniques to transform Ant System algorithms into two main parallel environments, namely the shared and distributed memory models. The \textit{Max-Clique} Ant System algorithm is chosen as our test bed due to the popularity of the \textit{Clique} problem and its throughout published results in~\cite{BR}. Our work convert the sequential Ant System algorithm into parallel environments, thus provide noticeable improvement in the algorithm running time while maintaining solution qualities comparable to ones obtained from the sequential version.
 %characteristics of dist / parallel
%todo: need other motivations other than speed improvement since 
%dist actually performs worse than seq!
\end{abstract}


\section{Introduction}\label{intro}


Many real world applications bear resemblances to combinatorial optimization problems in the $\cal NP$-hard class and thus, most likely they have no algorithms that give exact or even good approximate solutions in acceptable time, unless $\cal P = NP$.  Alternative strategies such as metaheuristic Ant based approaches have been shown to be very successful in these situations, even though they give no guarantee about solution qualities.  Nonetheless, in many cases Ant based methods still require a large computing resources and time, especially when the problem size increases.  Furthermore, ants inspired techniques attempt to imitate the social work of insects in nature, which inherently includes their concurrent operations.  These reasons provide the incentives for studies on Ant based algorithms in concurrent environments.

In previous paper, we used \textit{Ant System}, a variant of Ant based algorithms, to solve the Max-Clique problem~\cite{BR}. We also offer brief intuitions on a possible implementation of the algorithm in distributed computing.  In this paper, we describe the process of reimplementing our sequential algorithm with two popular parallel flavors, namely the shared and distributed memory models. We hope these ideas serve as proof of concepts that parallel computing is an ideal platform for general algorithms inspired by social insects.


The rest of the paper is organized as follows. In Section~\ref{background} we give preliminaries on Ant System, the Clique problem, and an overview of parallel computing.  We present our sequential and parallel Ant System algorithms for \textit{Max-Clique} in Section~\ref{algorithm} and compare their performance to the sequential one in Section~\ref{results}.  Finally, our conclusion and suggestions for future research are given in Section~\ref{conclusion}.

%, and the two parallel environments considered in this work

\section{Background}\label{background} 

Shared-memory environment consists of a single node with multiple cores or processors sharing 
the main memory 
while distributed-memory environment is a collection of multiple nodes, with each node 
contains a single processor with its own set of main memory. %is this line necessary ?


\subsection{Ant System algorithm}\label{max_clique} 

%todo: reduce these 3 paragraphs about AS to 2

%traditional ant based algorithm refer to ACO from Dorigo.  It is a heuristic techniques ...  

%ACO is the among the first design of Ant Based heuristic algorithms that imitate 
Both \textit{Ant System} (AS) and \textit{Ant Colony Optimization} (ACO)~\cite{DD} are heuristic techniques that imitate the collective behavior of ant colonies and their ability solve problems.  The inspiration of the design came from the observation that ants in a colony are able to find the shortest path to a food source by marking their trails with a chemical substance called {\it pheromone}~\cite{BDT, DG}.  These ants solve problems by passing messages to each other in the form of pheromone. Ants lay more or less pheromone according to how they perceive their conditions at a given time. Other ants arriving at a similar situation consider the amount of pheromone that has been laid, and use this to make informed decisions. 

%AS is a variant of Ant based system which also attemp to imitate the collective behavior of colonies and their ability to solve problems. 
%AS is also a meta heuristic model which also make uses the social networking of ants.  

Ant System differs from Ant Colony Optimization mainly in how the insects are used. In ACO, each ant produces a solution to the problem. The initial ant solves the problem almost purely stochastically. The subsequent ants use the information provided by the previous ants to produce better solutions. In AS, ants do not attempt to solve the problem individually. The solution to the problem is represented by the state of all ants in the colony captured in a snapshot of time.  Ant Colony Optimization excels in solving problems that are analogous to the shortest path problem (or the longest path problem, with minor modification to the heuristic). However, for problems that do not resemble a path optimization problem, the original ACO may not perform well~\cite{todo}.

Ant System has been shown to be effective in solving the clique, $k$-cardinality tree, graph coloring, and the bisection problems \cite{BR, BS2, CH,BS1}, while ACO has been used to solve problems like traveling salesman, quadratic assignment, routing, knapsack, among others \cite{DG, BJ2, MC}.


%todo: why choose this problem, see other papers to see why they choose certain problem
%search web for this problem is amenable to parallel computing because
%todo change the motivation to something obvious,  Kell'ers conjecture in gemoetry is so cryptic

\textbf{The \textit{Max-Clique} problem}: A \textit{clique} in graph $G$ is a complete sub-graph of $G$ in which there's an edge between any two vertices in the sub-graph.  The size of the clique is the vertices in that clique. The Max-Clique optimization problem asks for the \textit{largest} clique in a given graph.  In addition to being among the most popular and interesting problems in graph theory, Max-Clique is also useful in  solving real-life situations such as finding good codes, identifying faulty processors in multiprocessor systems, and finding counterexamples to Keller's conjecture in geometry \cite{BP2}\cite{Sloane}\cite{SMW}\cite{Keller}\cite{LS}.  Max-Clique belongs in the $\cal NP$-hard class of problems and thus, exact polynomial time algorithms for solving them are not expected to exist, unless $\cal P = NP$.  In fact, even finding a good approximation to an instance of Max-Clique is just as hard as finding an optimal solution~\cite{Hastad}. %Joe's 2004 paper

In 2004, Bui et al., provided a heuristic algorithm using Ant System for the Max Clique problem as given 
in [section Seq Approach].

%todo: words to use paradigm

\subsection{Parallel Computing}
These Ant based algorithms are very appealing to parallel computing because.  Morever Ant System is even more ammenable since the ants doesn't use local information and ....


The \textit{Shared-Memory} (SM) parallel model often refers to the symmetric multiprocessor (SMP) architecture which allows a collection of processors to share access to the main memory via buses. The differences and benefits of SM architecture to the distributed memory (DM) architecture used in previous section mainly come from the shared memory design that eliminates network communication overhead often experienced in DM\footnote{Concurrent computing overheads generally contain factors not in the sequential version and makes the parallel version inefficient. For instances: software overhead (e.g., time to create new threads), load balancing (unequal works), and communication overhead (data exchange time between processors)}. 

%benefit of SM
However the design of a \textit{SM} architecture is much more complicated and expensive than DM. In \textit{DM}, increasing the number of processors is as simply as adding new computers to the current environment. In \textit{SM} systems, e.g., SMP, increasing the number of processors or cores usually requires a new hardware design, e.g., motherboard, ~\cite{Hi}.  Nonetheless, multi-processor or core systems become more prevalent in recent years and thus induce redesigns from multiple software and programming domain to take advantage of multi-processing.



\section{Implementations of Ant System Algorithms}\label{algorithm}

\subsection{The Sequential Implementation}\label{par_app} 



\begin{center} 
\begin{algorithm}
\SetKwFor{For}{for}{do}{endfor}
\SetKwComment{Comment}{//}{}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\dontprintsemicolon

\SetLine
% {\bf{Algorithm ABAC}}$(G=(V,E))$  
\Input{Graph $G=(V,E)$}
\Output{A \myfont{clique} of $G$}
\medskip
\Begin{
  \medskip
  Use some heuristic method to get a \myfont{clique} $q$ in $G$\;
  Distribute \myfont{ants} on the vertices of $G$ with heavier concentration in $q$\; 
  \medskip
  \For{$s = 1$ {\bf{to}} \myfont{nStages}}{
    \For{$c = 1$ {\bf{to}} \myfont{nCycles}}{
      \For{$a = 1$ {\bf{to}} \myfont{nAnts}}{
        ant $a$ decides what to do\;
        ant $a$ executes its decision\;
      }
    }

    \medskip
    Find \myfont{cliques} through local optimization\;
    Save the \myfont{(cliques)} solutions\;
    Shuffle ants \Comment{perturbation}\;
  }

  \medskip
  \Return{best} \myfont{clique} found
  \medskip
}
\caption{The sequential Ant System algorithm for \textit{Max-Clique} (ASMC$_{seq}$)\label{fig:asmc_seq}} 
\end{algorithm}

\linespread{1.3}  
\end{center} 



  %Briefly describe the parts that require further explaination (e.g., FindClique is a greedy method . blah 
%todo:  in Bui et al's apper, nCycles is set to = , nAnts = , nStages =  , so the main program spends approximatels .... 
%say something about since nStates is small comparing to that of nNcycles * nAnts therefore we should parallelizing this section.

\subsection{The Shared-Memory Computing}\label{par_app} 




%todo: have to tie up everything to explain *why* shared memory is good, its benefits -- I have some above but doesn't seem enough.


\subsubsection{A Shared-Memory Implementation for ASMC}

The standard view of parallelism in shared-memory uses the \textit{fork/join} model depicted in Figure B. A single \textit{Master} thread is created when the program starts and this thread expands into multiple \textit{Slave} threads when parallelism is requested. These slave threads work concurrently (and independently) through the parallel region and merge back to the single master thread when they are done with the parallel section.

To parallel ASMC based on the \textit{fork/join} method, we need to identify candidate regions that can be paralleled. 
The main idea is to find section that can be parallelizing - of course have to make sure 
it doesn't affect the result quality.  So we can parallel the ant work part.

%see if I can find the html page where I have the result from gprof (profiler)

The works of the ants in Figure~\ref{fig:asmc_seq} is expanded and rewritten as shown in~\ref{fig:par_ops}

%note: do NOT say anything in details about edging and stuffs, just based on the algorithm above, some conflicts do occur however it is fine since ... explain in nature that's how ants make decisions, simultenously and allow some conflicts (not all, but some, it's exactly as in this algorithm).
%note: read this --- splitting the ant declision and ant move is actually a good idea - stick with it, since in real time, at each time step, the ant only knows what happens in the previous step -- not like the sequential method where at each cycle c, the last ant actually knows everything the other ants did at this *current* cyclec.  Thus  this is the reason why we opt for this parallelism approach.  Since it conforms with nature




\begin{center} 
  \begin{algorithm}
    \SetKwFor{For}{for}{do}{endfor}
    \SetKwComment{Comment}{//}{}
    \dontprintsemicolon

    \SetLine
    \Begin{
      \For{$c = 1$ {\bf{to}} \myfont{nCycles}}{
        \medskip
        \Comment{Ants make decisions in \myfont{parallel}}

        \For{$a = 1$ {\bf{to}} \myfont{nAnts}}{
          ant $a$ heuristically decides its next destination, vertex v$_a$, based on surrounding \myfont{pheromone} information\;
        }

        \medskip

        \Comment{Ants move in \myfont{parallel}}
        \For{$a = 1$ {\bf{to}} \myfont{nAnts}}{
          ant $a$ moves to new destination, vertex v$_a$, and deposits \myfont{pheromone} along the way\;
        }
        \medskip
      }

    }
    \caption{Parallel Ant Operations\label{fig:par_ops}} 
  \end{algorithm}

  \linespread{1.3}  
\end{center} 


%todo mentioned we can try this on Coloring etc

\subsection{The Distributed Implementation}\label{dist_app} 

\label{distributed}

ASMC is especially suited to distributed implementation since ants contribute
partial solutions based on local information.  Distribution makes it possible
to optimize the algorithm for speed, due to parallel processing, or for space,
due to partitioning ants and large graphs over several machines.  Though the
benefits gained depend on the specific implementation, ASMC itself does not
preclude any of the benefits.


%mention in this algorithm, distributed doesn't help much in *speed* since 
%lots of depending steps where the clients have to send results back to server
%before they can continue, however it saves space and another intuition for 
%using dist is that we can test different type of network settings to 
%see how the ants behave, for instance instead of one large colony, we can 
%have multiple colonies (each run on a node), an one colony might behave different from another to 
%increase diversity.  We can also test the network types such as scale free, 
%small-world to see if that affects the performance.  In short, we conjecture 
%that dist is used to increase the quality of the solution (and not speed).

%nice I can say this dist alg is just a proof of concept that it works !
%future works will involve diff network settings etc etc.


%generalize this, have  c clients and m masters,  in this experiment we use c = 4, 
%and m = 1, it would be interesting to use other settings that resemble other network topoligies
%.. scale free whatever.

The distributed implementation built for this paper is a simple proof of
concept and, as such, performs synchronous inter-process communication through a
central server.  Ants are distributed across four machines, each of which has
a complete copy of the graph.  The idea is for ants to move from processor to
processor.  One machine is designated the server and the others clients 1, 2
and 3.  The server coordinates four synchronous transactions: starting, ant
transfer, local optimization and ending.

\begin{itemize}

\item To {\bf start}, the server first waits for each client to connect.  It
then partitions the vertices into four roughly equal sets and assigns each set
to a processor, including itself.  It sends each client a list of which
vertices are ``owned'' by which processors.

\item {\bf Ant transfer} occurs at the end of each stage.  The server asks
each client for a list of ants that are moving to other processors.  It sorts
these ants according to their destinations and sends each client a list of
incoming ants.  Each processor instantiates the ants on its copy of the graph.
It also updates cached data regarding vertices and edges connected to its
``owned'' vertices.  The cached data makes it possible for ants to make
informed decisions about whether to move to another processor in the future.

\item {\bf Local optimization} occurs after each ant transfer.  Each client
sends the server vertex and edge data, and the server performs the same
operations as in the sequential implementation of ASMC.  In principle, this
step could be redesigned to be less centralized.

\item The server {\bf ends} by letting each client know that the run has
ended.  Synchonized starting and ending makes it simpler to invoke the program
from a script.

\end{itemize}





Though this implementation is rather simple, it serves as our proof of concepts that distributed computing is applicable to Ant System.  We'll provide results for the implement in~\ref{} and discuss other possible designs for the algorithm.



\section{Experimental Results}\label{results} %todo: REDUCE this, see dam-1 paper for example, only 1 paragraph, this should be no more than 3 paragraphs !!!  

% In this section we present the results of our algorithm on 119 benchmark
% graphs given at http://mat.gsia.cmu.edu/COLOR04/.  Information about these
% graphs is summarized in Table~\ref{tab:info}.  The algorithm was implemented
% in C++ and run on a 3.2 GHz Mobile Pentium4 PC with 1 GB of RAM running the
% Linux operating system.  The machine benchmark is given at the end of the
% paper in Figure~\ref{fig:benchmark}.  For each of the 119 graphs in
% Table~\ref{tab:info} we ran our algorithm for 50 trials.  Of the 119 graphs
% there are 63 graphs with either known chromatic number or best known bound on
% the chromatic numbers.  Of these 63 graphs, our algorithm found matching
% bounds for 56 of them. There are 7 graphs for which our algorithm got poorer
% results, but are within 1 of the best known bound.  The results are summarized
% in Tables~\ref{tab:results1} and \ref{tab:results2}.  For each graph, we list
% the name of the graph, the chromatic number or the best known bound on the
% chromatic number, the minimum, maximum, average and standard deviation of the
% results produced by our algorithm in 50 trials.  We also list the average
% running time (in seconds) out of the 50 runs of each graph.  For a number of
% graphs the running times were too small to be recordable and were recorded as
% 0.  It should be noted that the standard deviations of the results are quite
% small, less than 1 for all but three graphs.  For the remaining three graphs
% the standard deviations are less than 2.



\textbf{Shared-Memory implementation}: We use C++ with OpenMP support in our parallel Shared-Memory implementation for ASMC. The OpenMP (Multi-Processing) API~\cite{OpenMP} supports C, C++, and FORTRAN in all platforms and consists of compiler directives and library functions to assist compilers to execute instructions using the \textit{fork/join} model described in Section~\ref{par_app}.  Our development environment is on a multi-processor computer provided by the HP Test-Drive~\cite{HP_Testdrive} program. The system is equipped with sixteen Intel Itanium $1.5$ GHz processors, with $8$ GB of Ram, running Linux with the Intel Compiler suite. 
%todo: The machine benchmark is given at the end of the paper in Figure~\ref{fig:benchmark}



Tables~\ref{tab:spar_table} and~\ref{tab:spar_su_par_table} respectively show our OpenMP parallel results and the speed-up $S(P_d)$ obtained with different number of processors\footnote{$S(P_d) = \frac{T(P_1)}{T(P_d)}$, $T(P_1)$ is the sequential running time (e.g., on a single processor), $T(P_d)$ is the running time with $d$ processors.}. For each of the benchmark graphs we ran our algorithm for $100$ trials, and these trials are repeated $100$ times for different number of processors $P_{d=1,2,4,8,16}$.  Table~\ref{tab:spar_table} shows that the results from the parallel implementation have similar qualities as those from the sequential version.  In Table~\ref{tab:spar_su_par_table} we observe some parallel running times on simple instances are worse than those of sequential ones. This is as expected since the overhead caused by the fork/join operations in OpenMP dominates the program short running time (less than a second).  However the benefits of parallelism in shared-memory show on larger instances.  Even for medium size tasks (e.g., \textit{keller\_5}) taking just about $90$ seconds, we achieved more than $50\%$ run time improvement with dual processors.  On the two largest graph instances, \textit{MANN\_45} and \textit{HAMMING10-2}, we obtained promising speed-up's around $2, 3.5, 6, 9$ with $2,4,8,16$ processors, respectively.


Though the implementation is simple we achieve our goal of speeding up the running time, for future work we can hybridize this with distributed computing.  %more on this


\linespread{1}
\begin{table*}[ht!]
\caption{Solution Quality\label{tab:spar_table}}
\begin{footnotesize}
\begin{center}
\begin{tabular}{|l||c|c||c|c|c|c|c|}
\hline%todo have to explain P Best -- use a dag or something
&&$P$& \multicolumn{5}{|c|}{Solution Avg/StdDev when run on $P_d$ processors}\\
\cline{4-8}
Graph &Opt & Best      &$P_1$(Seq)  &$P_2$ &$P_4$   &$P_8$    &$P_{16}$\\
\hline											            				       
c-fat200-1 	&12 &12	   &12.00/0.00 &12.00/0.00 &12.00/0.00 &12.00/0.00 &12.00/0.00\\
c-fat500-1 	&14 &14	   &14.00/0.00 &14.00/0.00 &14.00/0.00 &14.00/0.00 &12.00/0.00\\
%% \hline
johnson16-2-4 	&8 	&8 	   &8.00/0.00 &8.00/0.00 &8.00/0.00 &8.00/0.00 &8.00/0.00\\
johnson32-2-4 	&16 &16    &16.00/0.00 &16.00/0.00 &16.00/0.00 &16.00/0.00 &16.00/0.00\\
\hline 
keller4 &11 &11 	&10.22/0.83 &10.2/0.87 &10.35/0.84 &10.24/0.86 &10.44/0.82\\
keller5 &27 &24     &21.28/0.87 &21.45/0.95 &21.37/0.92 &21.4/0.82 &21.46/8.89\\
%% \hline
hamming10-2 &512 &512 &512.00/0.00 &512.00/0.00 &512.00/0.00 &512.00/0.00 &512.00/0.00\\
hamming8-2 	&128 &128 &128.00/0.00 &128.00/0.00 &128.00/0.00 &128.00/0.00 &512.00/0.00\\
\hline
san200\_0.7\_1 	&30 &30 &20.98/5.95 &21.4/6.09 &21.4/5.99 &19.24/4.62 &20.43/5.53\\
san200\_0.9\_1 	&70 &70 &45.52/0.67 &45.58/0.62 &45.64/0.76 &45.51/0.66 &45.64/0.7\\
san200\_0.9\_2 	&60 &60 &39.59/2.31 &39.58/2.75 &39.56/2.62 &39.84/3.07 &39.99/4.21\\
san200\_0.9\_3 	&44 &37 &34.61/0.9 &34.83/0.98 &34.82/0.92 &34.88/1.14 &34.91/1.08\\
san400\_0.5\_1 	&13 &13 &8.67/0.79 &8.72/0.9 &8.69/0.78 &8.73/0.79 &8.63/0.66\\
san400\_0.9\_1 	&100 &100 &68.25/20.35 &75.66/22.44 &74.19/21.77 &73.01/21.79 &71.44/21.69\\
\hline
sanr200\_0.7 	&18 &18  &17.27/0.55 &17.22/0.5 &17.29/0.62 &17.34/0.62 &17.34/0.51\\
sanr400\_0.5 	&13 &13  &12.06/0.4 &12.06/0.37 &12.01/0.41 &12.01/0.48 &11.98/0.47\\
san1000 	    &15 &10  &9.94/0.24 &9.96/0.2 &9.94/0.24 &9.93/0.26 &9.97/0.17\\
\hline
brock200\_1 	&21 &21  &19.48/0.52 &19.5/0.5 &19.55/0.54 &19.57/0.53 &19.64/0.52\\
brock400\_1 	&27 &25  &22.71/0.73 &22.75/0.67 &22.82/0.75 &22.82/0.74 &22.75/0.82\\
brock800\_1 	&23 &21  & 18.49/0.62 &18.61/0.66 &18.59/0.6 &18.7/0.66 &18.67/0.65\\
\hline
p\_hat300\_1 	&8 	&8   &8/0 &7.99/0.1 &7.99/0.1 &7.99/0.1 &8/0\\
p\_hat300\_2 	&25	&25  &25.00/0.00 &25.00/0.00 &25.00/0.00 &25.00/0.00 &25.00/0.00\\
p\_hat300\_3 	&36 &36  &34.89/0.87 &35.1/0.89 &35.01/0.89 &35.12/0.86 &34.97/0.98\\
p\_hat500\_1 	&9 	&9   &8.94/0.24 &8.98/0.14 &9/0 &8.98/0.14 &9/0 \\
p\_hat500\_2 	&36 &36  &35.96/0.2 &35.98/0.14 &35.92/0.27 &35.93/0.26 &35.93/0.26\\
p\_hat700\_1 	&11 &11  &9.31/0.61 &9.31/0.52 &9.27/0.55 &9.37/0.67 &9.24/0.51\\
p\_hat1000\_1 	&10 &10  &9.95/0.22 &9.99/0.1 &9.98/0.14 &9.98/0.14 &9.94/0.24\\
p\_hat1500\_1 	&12 &11	 &10.67/0.47 &10.72/0.45 &10.66/0.47 &10.72/0.45 &10.73/0.44\\
\hline
MANN\_a27 	&126 &126 &125.00/0.00 &125.01/0.10 &125.00/0.00 &125.01/0.10 &125.00/0.0\\
MANN\_a45 	&345 &342 &342/0.00 &342/0.00 &342/0.00 &342/0.00 &342/0.00 \\
\hline
%\multicolumn{8}{@{}l}{\tiny $^*$Results for 100 runs per graph.}
%\qquad\qquad $^\dag$All times are in seconds.}\\
%\multicolumn{9}{@{}l}{\tiny $^\dag$All times are in seconds.}\\
\end{tabular}
\end{center}
\end{footnotesize}
\end{table*}
\linespread{1.3}


\linespread{1}
\begin{table*}[ht!]
%\caption{Time and Speedup$^{*\dag}$\label{tab:spar_su_par_table}}
\caption{Time and Speedup\label{tab:spar_su_par_table}}
\begin{footnotesize}
\begin{center}
\begin{tabular}{|l||c||c|c|c|c|}
\hline
&              & \multicolumn{4}{|c|}{Running Time Avg/Speed-up when run on $P_d$ processors}\\
\cline{3-6}
Graph&Veritices/Edges& 	P$_2$           &P$_4$            &P$_8$            &P$_{16}$   \\
\hline										              			          
c-fat200-1 	    &200/1534 		     &00.94/0.69 &00.73/0.89 &00.84/0.78 &00.84/0.78\\
c-fat500-1 	    &500/4459 		     &02.47/0.79 &02.03/0.97 &01.97/1.00 &02.07/0.95\\
\hline                                                                                   
johnson16-2-4 	&120/5460 		 &01.65/0.97 &01.24/1.29 &01.11/1.44 &01.01/1.59\\
johnson32-2-4 	&496/107880 	 &33.32/1.24 &22.18/1.87 &15.55/2.67 &11.69/3.55\\
\hline                                                                                   
keller4      	&171/9435 			 &P2/     &P4/     &P8/   ) &P16  /)\\  
keller5 	    &776/225990 		 &120.91/1.64 &71.14/2.79 &44.57/4.45 &30.52/6.49\\
\hline                                                                                   
hamming10-2 	&1024/518656 	 &503.04/1.81 &277.57/3.29 &159.88/5.7  &97.85/9.32\\
hamming8-2 	    &256/31616 	     &09.89/1.15 &06.54/1.74 &04.85/2.35 &03.81/2.99\\
\hline                                                                                   
san200\_0.7\_1 	&200/13930 	     &4.20/1.01 &2.84/1.49 &2.40/1.77 &2.07/2.04\\
san200\_0.9\_1 	&200/17910 	     &5.62/1.07 &3.73/1.61 &3.01/1.99 &2.50/2.39\\
san200\_0.9\_2 	&200/17910 	     &5.52/1.04 &3.69/1.56 &2.96/1.94 &2.48/2.32\\
san200\_0.9\_3 	&200/17910 	     &5.39/1.02 &3.66/1.50 &2.98/1.84 &2.41/2.29\\
san400\_0.5\_1 	&400/39900 	     &11.66/1.14 &07.73/1.72 &06.07/2.19 &04.89/2.71\\
san400\_0.9\_1 	&400/71820 	     &21.25/1.12 &14.05/1.70 &10.25/2.33 &07.95/3.00\\
\hline                                                                                   
sanr200\_0.7 	&200/13868 	     &4.09/1.03 &2.84/1.49 &2.4/1.76 &2.07/2.04\\
sanr400\_0.5 	&400/39984 	     &11.92/1.09 &8.11/1.61 &6.17/2.11 &5.02/2.59\\
san1000 	    &1000/250500 	 &127.43/1.65 &73.77/2.85 &46.96/4.48 &32.44/6.49\\
\hline                                                                                   
brock200\_1 	&200/14834 	     &4.27/1.06 &3.01/1.51 &2.57/1.77 &2.17/2.09\\
brock400\_1 	&400/59723 	     &17.76/1.11 &11.7/1.69 &8.73/2.27 &6.80/2.91\\
brock800\_1 	&800/207505 	 &102.51/1.63 &61.14/2.73 &39.24/4.25 &27.17/6.14\\
\hline                                                                                   
p\_hat300\_1 	&300/10933       &3.99/1.10 &2.81/1.56 &2.43/1.8  &2.17/2.02\\
p\_hat300\_2 	&300/21928       &7.42/1.14 &4.88/1.73 &3.83/2.2  &3.22/2.62\\
p\_hat300\_3 	&300/33390       &9.56/1.14 &6.61/1.64 &5.13/2.12 &4.14/2.62\\
p\_hat500\_1 	&500/31569       &10.74/1.20 &7.36/1.76 &5.90/2.19 &4.82/2.68\\
p\_hat500\_2 	&500/62946       &21.86/1.23 &14.08/1.91 &10.18/2.64 &7.91/3.39\\
p\_hat700\_1 	&700/60999       &22.03/1.26 &14.45/1.92 &10.83/2.56 &8.59/3.23\\
p\_hat1000\_1 	&1000/122253      &49.35/1.37 &30.56/2.22 &21.72/3.12 &16.16/4.20\\
p\_hat1500\_1 	&1500/284923      &209.25/1.75 &117.98/3.10 &72.60/5.04 &47.47/7.70\\
\hline                                                                                   
MANN\_a27    	&378/70551 	         &21.11/1.19 &14.01/1.80 &9.98/2.52 &7.52/3.35\\
MANN\_a45    	&1035/533115 	     &474.11/1.81 &258.94/3.30 &152.74/5.60 &94.87/9.02\\
\hline
%\multicolumn{9}{@{}l}{\tiny $^*$Results for 100 runs per graph.
%\qquad\qquad $^\dag$All times are in seconds.}\\
\end{tabular}
\end{center}
\end{footnotesize}
\end{table*}
\linespread{1.3}





\textbf{Distributed-Memory implementation}: Our Distributed-Memory implementation of ASMC was run on four Sun Blade $100$  workstations using standard Unix socket communication.  Each machine has $450$ Mhz CPU, todo MB memory, and runs Sun OS.  The \textit{DFMAX} $(r500.5.b)$ benchmark for the machine is 218.69 (user), 0.03 (sys), 366.00 (real).%memory, operating system etc
%Todo: reasons for using socket communication and not MPI

%tvn: benchmark info was taken from ~/Documents/Code/Web/www_2/www/Computer/Projects/Color/Files/test_sol/benchmark/results.sparc10.41

Table~\ref{tab:distributed1} summarizes the results of our Distributed-Memory implementation of ASMC. Due to time limitations we were able to run the algorithm only on a subset of the $30$ test graphs.  Also, we ran the algorithm $100$ times for each graph.  The results are comparable to that of the sequential implementation. 


In this proof of concept implementation, we show that the solution quality of the program doesn't degrade when run in distributed computing.  The main benefit is that we are able to partition the program data into smaller size ... any other benefits.  In future works, we plan to use Message Passing Interface (MPI) method for improvement of communication time and to exploit the how network topologies affect the performance. 




For example, one could build a less centralized, peer-to-peer
model.  The ant transfers could occur asynchronously throughout each stage.
The graph partitions could adapt over time to minimize the number of shared
edges.  The graph itself could be distributed so that each client is
completely unaware of vertices it does not ``own,'' thereby enabling runs on
extremely large graphs in a reasonable time.



%todo: provide a note that machines are different so hard to get accurate rate- however
%the benchmark results are given.  
%also could reduce the # of instances test so every implementations (result tables) are 
%consistent









\begin{table*}[h!]
\caption{ASMC$_d$ results (Distributed Implementation)\label{tab:distributed1}}
\begin{footnotesize}
\begin{center}
\begin{tabular}{|l||c|c|c||c|c|c|}
\hline
&\multicolumn{3}{|c||}{ASMC$_{seq}$}&\multicolumn{3}{|c|}{ASMC$_{dist}$}\\
\cline{2-7}
Graph&\multicolumn{2}{|c|}{Solution}&Time&\multicolumn{2}{|c|}{Solution}&Time\\
\cline{2-3}  \cline{5-6}  
                &Best&Avg/StdDev&Avg$^*$ & Best  & Avg/StdDev     	&  Avg$^\dag$     \\

\hline				            				       
c-fat200-1    	&12&12.00/0.00& 0.46 & 12 	& 12.00/0.00 	& 0.72     \\
c-fat500-1      &14&14.00/0.00& 1.55 & 14    & 14.00/0.00    & 2.61     \\
\hline
johnson16-2-4 	& 8& 8.00/0.00& 1.58 & 8 	& 8.00/ 	0.00 	& 1.37     \\
\hline
keller4 	    &11&10.44/0.76& 3.16& 11 	& 10.49/0.64 	& 3.21     \\
\hline
hamming8-2 	    &128&128.00/0.00&11.20 & 128 	& 127.26/   2.78 	& 32.70    \\
\hline
san200\_0.7\_1 	&30&18.81/5.35& 4.85 & 30 	& 17.16/1.93 	& 6.29     \\
san200\_0.9\_1 	&70&47.72/4.18& 5.75 & 48 	& 47.09/0.40 	& 10.64     \\
\hline
sanr200\_0.7 	&18&15.32/0.83&4.80 & 17 	& 15.44/0.69 	& 6.22     \\
\hline
brock200\_1 	&20&17.98/0.93&5.12 & 20 	& 18.49/0.64 	& 7.21     \\
\hline
p\_hat300\_1 	&8&7.17/0.38&4.26 & 8 	& 8.00/0.00 	& 4.52     \\
p\_hat500\_1 	&9&8.19/0.47&14.19 & 9 	& 8.11/0.31 	& 33.60    \\
\hline
\multicolumn{6}{@{}l}{\tiny $^*$ASMC$_{seq}$ was run on Pentium 4 2.4 Ghz architecture}\\
\multicolumn{6}{@{}l}{\tiny $^\dag$ASMC$_d$ was run on Sun Blade 450 Mhz architecture}\\
\end{tabular}
\end{center}
\end{footnotesize}
\end{table*}

%IMPORTANT: the SEQ was implemented with a MUCH MUCH faster computer



%tvn test

% \linespread{1}
% \begin{table*}[ht!]
% \caption{Solution Quality, Sequential vs Parallel implementation\label{tab:spar_table}}
% \begin{footnotesize}
% \begin{center}
% \begin{tabular}{|l||c|c||c|c|c|c|c|}
% \hline%todo have to explain P Best -- use a dag or something
% Graph&Opt&$P$& \multicolumn{5}{|c|}{Solution Avg/StdDev when run on $P_d$ processors}\\
% &  &Best      &$P_1$  &$P_2$ &$P_4$   &$P_8$    &$P_{16}$\\

% \hline											            				       
% c-fat200-1 	&12 &12	   &12.00/0.00 &12.00/0.00 &12.00/0.00 &12.00/0.00 &12.00/0.00\\
% \hline
% \multicolumn{8}{@{}l}{\tiny $^*$Results for 100 runs per graph.}
% \end{tabular}
% \end{center}
% \end{footnotesize}
% \end{table*}
% \linespread{1.3}

% %% Table 3
% \linespread{0.7} 
% \begin{table}[h!]
% \begin{center} \small 
% \begin{tabular}{|l|r|r|r|r|r|r|}
% \hline 
% & & \multicolumn{5}{|c|}{50 runs of ABAC on each instance}\\
% \cline{3-7}
% Instances &  Best &  Min & Max & Avg & SD & Avg.  Time (s) \\ 
%  &  Known &   &  & &  & \\ 
% \hline
% &&&&&& \\
% le450\_15d.col.b         &  15     &  15      &  21     &  17.02   &  1.42   &  42.66     \\
% &&&&&& \\
% \hline 
% \end{tabular} 
% \caption{ABAC results for instances from the COLOR04 web site (cont.)}\label{tab:results2}
% \end{center} 
% \end{table}
% \linespread{1.3}




\section{Conclusion and Future Research}\label{conclusion}

%todo: the sequential ASMC version has been extensively tested on a variety of DIMACS benchmark graphs %[ref].  In this section we give the results on how the two parallel implementation of ASMC perform.


In this paper, we provide implementations of our Ant System algorithm for Max-Clique in three computing environments: sequential, shared, and distributed memory.   We maintained the solution qualities of sequential algorithm while achieved very promising speed-up in the shared memory model. In limited experiments, we found our distributed memory version reduces the work load and space usage in each computer while preserving comparable results to that of the sequential version.

Our future objective is using parallel computing to obtain better quality solutions -- instead of just raw running time.  For example having colonies of ants work independently from one another and exchange information periodically or analyzing how different network topologies of ant colonies affect the general solution quality. These ideas will be investigated in more thorough in future studies.  %todo: reword this, combine it with the stuffs I said at the end of experimential results (don't separate them into two sections)

%todo: say something although there are many other recent methods such as ref ref 
%giving better results than ASMC, our purpose is not to compare Ant System with those
%approaches but rather provide parallelization concepts
%techniques to Ant System based algorithms.


%todo: words to use: intuiations etc 
%todo: install flyspell

%todo: literatures ! 1) All the parallel stuffs 2) recent techniques for Max Clique (should be short) - 

%todo: somehow have to describe the ideas that multicore: speed, multi machine: intelligence
%todo: any other type of graph, visual stuffs that might help ?  See other *related* papers 



\section{Acknowledgements}

The authors would like to thank the anonymous referees for their valuable comments.

\begin{thebibliography}{99} 

% \bibitem{ACCOV}
% J. Abril, F. Comellas, A. Cortes, J. Ozon and M. Vaquer,
% ``A Multi-Agent System for Frequency Assignment in Cellular Radio
% Networks'',
% IEEE Transactions on Vehicular Technology,  49(5) September 2000, pp.
% 1558--1564.


% \bibitem{BGS}
% M. Bellare, O. Goldreich and M. Sudan,
% ``Free Bits, PCPs and Non-Approximability - Towards Tight Results'',
% SIAM J. Comp. 27, 1998, pp. 804--915.

% \bibitem{BK}
% A. Blum, D. Karger
% ``An $O(n^{3/14})-$Coloring Algorithm for 3-Colorable Graphs'',
% {\it Information Processing Letters}, 61(1), January 1997. pp. 49--53.

\bibitem{BDT} %us
E. Bonabeau, M. Dorigo and G. Theraulaz,
``Inspiration for Optimization from Social Insect Behavior'',
Nature, Vol. 406, July 6, 2000, pp. 39--42.


% \bibitem{B}
% D. Brelaz,
% ``New Methods to Color the Vertices of a Graph'',
% Communications of the ACM, 22(4),  April 1979, pp. 251--256.

% \bibitem{BP}
% T. N. Bui and C. Patel,
% ``An Ant system Algorithm for Coloring Graphs'',
% {\it Computational Symposium on Graph Coloring and its Generalizations}, 
% COLOR02,
% Cornell University, September 2002.


\bibitem{BR} %use
T.N. Bui and J.R. Rizzo, ``Finding Maximum Cliques with Distributed Ants,'' Proceedings of the Genetic and Evolutionary Computation Conference, 2004, pp. 24--35.

\bibitem{BS2}
T.N. Bui  and G. Sundarraj,  ``Ant System for the $k$-Cardinality Tree Problem,'' Proceedings of the Genetic and Evolutionary Computation Conference, 2004, pp. 36--47.

\bibitem{BS1}
T.N. Bui and L.C. Strite, ``An Ant System Algorithm for Graph Bisection,'' Proceedings of  Genetic and Evolutionary Computation Conference, 2002, pp. 43--51.


% \bibitem{CS}
% M. Chiarandini, T. Stutzle,
% ``An Application of Iterated Local Search to Graph Coloring Problem'',
% {\it Computational Symposium on Graph Coloring and its Generalizations}, 
% COLOR02,
% Cornell University, September 2002.


% \bibitem{CO}
% F. Comellas and J. Ozon,
% ``Graph Coloring Algorithms for Assignment Problems in Radio Networks,''
% Applications of Neural Networks to Telecommunications 2, 1995, pp.
% 49--56.


% \bibitem{CO2}
% F. Comellas and J. Ozon,
% ``An Ant Algorithm for the Graph Coloring Problem,''
% ANTS'98 -- From Ant Colonies to Artificial Ants: First International 
% Workshop on Ant Colony Optimization, Brussels, Belgium, October 15-16, 1998.

% \bibitem{CLGA}
% C. Coritoru, H. Luchian, O. Gheorghies, A. Apetrei,
% ``A New Genetic Graph Coloring Heuristic'',
% {\it Computational Symposium on Graph Coloring and its generalizations}, 
% COLOR02,
% Cornell University, September 2002.

\bibitem{CH}%use
D. Costa and A. Hertz,
``Ants Can Colour Graphs,''
Journal of Operational Research Society,  Vol. 48, 1997, pp. 295--305.

% \bibitem{CL}
% J. Culberson, F. Luo,
% ``Exploring the k-colorable landscape with Iterated Greedy'',
% {\it Cliques, Coloring and Satisfiability -- Second DIMACS Implementation
% Challenge 1993}, American Mathematical Society, Volume 26 (1996), pp. 
% 245--284.

\bibitem{DD} %use
M. Dorigo and G. Di Caro,
``The Ant Colony Optimization Meta-Heuristic,''
{\it New Ideas in Optimization}, McGraw-Hill, 1999, pp. 11--32.


\bibitem{DG}%use
M. Dorigo and L. Gambardella,
``Ant Colony System: A Cooperative Learning Approach to the Traveling
Salesman Problem,''
IEEE Trans. on Evol. Computation, 1(1), 1997, pp. 53--66.

\bibitem{FF}
C. Fleurent, J. Ferland,
``Genetic and Hybrid Algorithms for Graph Coloring'',
{\it Annals of Operations Research}, Volume 63, 1996, pp. 437--461.

\bibitem{GHZ}
P. Galinier, A. Hertz, N. Zufferey,
``Adaptive Memory Algorithms for Graph Coloring'',
{\it Computational Symposium on Graph Coloring and its Generalizations}, 
COLOR02,
Cornell University, September 2002.

% \bibitem{GH}
% P. Galinier ad J. Hao,
% ``Hybrid Evolutionary Algorithms for Graph Coloring'',
% {\it Journal of Combinatorial Optimization}, October 1998.

% \bibitem{GD}
% L. M. Gambardella and  M. Dorigo,
% ``An Ant Colony System Hybridized with a New Local Search for the Sequential
% Ordering Problem,''
% INFORMS Journal on Computing, 12(3), Summer 2000.


% \bibitem{GPR}
% F. Glover, M.Parker, J. Ryan,
% ``Coloring by Tabu Branch and Bound'',
% {\it Cliques, Coloring and Satisfiability -- Second DIMACS Implementation
% Challenge 1993}, American Mathematical Society, Volume 26 (1996), pp. 
% 285--307.

% \bibitem{GOM}
% C. Gomes, D. Shmoys,
% ``Completing Quasigroups or Latin Squares: A Structured Graph Coloring
% Problem",
% {\it Computational Symposium on Graph Coloring and its generalizations}, 
% COLOR02,
% Cornell University, September 2002.

% \bibitem{HWe}
% A. Hertz, D. Werra,
% ``Using Tabu Search Techniques for Graph Coloring'',
% {\it Computing}, Volume 39, 1987, pp. 345--351.

% \bibitem{H}
% M. M. Halld\'orsson,
% ``A Still Better Performance Guarantee for Approximate Graph Coloring'',
% Information Processing Letters, 45, 1993, pp. 19--23.


% \bibitem{JT}
% T. R. Jensen and B. Toft,
% {\it Graph Coloring Problems,} Wiley-Insterscience Series in Discrete
% Mathematics and Optimization, 1995.


% \bibitem{JAMS}
% D. Johnson, C. Aragon, L. McGeoch and C. Schevon,
% ``Optimization by Simulated Annealing: An Experimental Evaluation; Part II,
% Graph Coloring and Number Partitioning,''  
% Operations Research,  39(3), May--June 1991, pp. 378--406.


% \bibitem{JTr}
% D. S. Johnson and M. A. Trick (Editors),
% {\it Cliques, Coloring and Satisfiability -- Second DIMACS Implementation
% Challenge 1993,}  
% DIMACS Series in Discrete Mathematics and Theoretical Computer Science,
% American Mathematical Society, Volume 26 (1996).

% \bibitem{LC}
% G. Lewandowski, A. Condon,
% ``Experiments with Parallel Graph Coloring Heuristics and Applications of
% Graph Coloring'',
% {\it Cliques, Coloring and Satisfiability -- Second DIMACS Implementation
% Challenge 1993}, American Mathematical Society, Volume 26 (1996), pp. 
% 309-334.

% \bibitem{L}
% F. T. Leighton,
% ``A Graph Coloring Algorithm for Large Scheduling Problems",
% Journal of Research of the National Bureau of Standards, 84(6), 1979, 
% pp. 489--506.

\bibitem{MC} %use
V. Maniezzo and A. Carbonaro,
``Ant Colony Optimization: An Overview'', {\it Essays and Surveys in
Metaheuristics}, C. Ribeiro editor, Kluwer Academic Publishers, 2001, pp.
21--44.


% \bibitem{MN}
% K. Mizuno, S. Nishihara,
% ``Toward Ordered Generation of Exceptionally Hard Instance for Graph
% 3-Colorability'',
% Computational Symposium on Graph Coloring and its Generalizations, 
% COLOR02, Cornell University, September 2002.

% \bibitem{Mor}
% C. Morgenstern,
% ``Distributed Coloration Neighborhood Search'',
% {\it Cliques, Coloring and Satisfiability -- Second DIMACS Implementation
% Challenge 1993}, American Mathematical Society, Volume 26 (1996), pp. 
% 335--358.

% \bibitem{PS}
% V. Phan, S. Skiena,
% ``Coloring Graphs with a General Heuristic Search Engine''
% Computational Symposium on Graph Coloring and its Generalizations, 
% COLOR02, September 2002.


% \bibitem{WPO}
% T. White, B. Pagurek, F. Oppacher,
% ``ASGA: Improving the Ant System by Integration with Genetic Algorithms'',
% Proceedings of the 3rd Conference on Genetic Programming (GP/SGA 98), 
% July
% 1998, pp. 610--617.


\end{thebibliography}



%\null


\begin{figure}[th!]

\hrule

\medskip

The following data was obtained after DFMAX was recompiled on the
machine that we tested our algorithm.

\begin{tabular}{@{\qquad\qquad}l}
DFMAX(r500.5.b)\\
5.67 (user)\quad      0.00 (sys)\quad      6.00 (real)\\
Best: 345 204 148 480 16 336 76 223 260 403 141 382 289\\
\end{tabular}
\medskip
\hrule
\medskip
\caption{Machine Benchmark\label{fig:benchmark}} 
\end{figure}



  

\end{document}



%\usepackage{fancyvrb} 
%\usepackage[ruled,vlined]{algorithm2e}
